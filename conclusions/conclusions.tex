\section{Conclusions}\label{sec:conclusions}
In terms of performance of the separation constraint in comparison to batchnorm as an enhancement to \ReLU networks, we find common ground with the thoughts presented by Hasanpour et al. in the \texttt{SimpNet} paper \cite{simpnet} with regards to an a-priori relation between depth and width. While Hasnpout et al. suggest that as networks grow deeper, so should the width. We provide an interpretation of it. For a rectangular network of fixed with $W$ and depth $D$, let $A_{F,X}(W,D)$ denote the accuracy reached by network $F$ (in the sense of section \ref{sec:separability}) over dataset $X$. Then the possible configurations
\\\\
We would like to remark that the Separation constraint are effective either using fully-connected or convolutional layers, see Section \ref{sec:experiments}.
\\\\
We find that although the Separation constraints enable to train deeper networks, this improvement is not absolute and eventually it fails. However, it is able to improve Batch Normalization by tranining using the minimum width twice as deep. Additionally, we find experimentally that it also induces unstabilities during training which display interesting properties like increasingly complex but smoother boundaries which indeed require of further research.
\\\\
However, further inquiry into the interaction between constraint loss and main loss beyond parameter $\lambda$ is needed, alongside with experiments regarding other losses besides cross-entropy. In addition,  the use of the separation constraints in other types of layers such as LSTM \cite{lstm} or transformers \cite{transformer}\cite{transformer2} is yet to be performed, in order to explore other types of tasks like regression, unsupervised learning \cite{embedding}, graph \cite{graph} or generation \cite{gan,vae}. Morever, more challenging datasets are definitely in need of revision.
\\\\

% 4.1
While the separation constraints prevent the vanishing gradient, the \emph{exploding gradient} remains at large. Notice that constraint introduction places \emph{lower bounds} on the magnitude of the pre-activation values, but does not place upper bounds. This could be solved introducing additional constraints forcing them. 





