\section{Conclusions}\label{sec:conclusions}
Through the \SepConstraint family, we have shown that deeper networks can be trained without resorting to any architectural modifications or data manipulations. Moreover, it can be done to surpass Batch Normalization. The lesson to be learned here is that the training of deeper networks can be achieved \emph{accomodating} the network for data, rather than manipulating it.
\\\\
In addition, our use of the grids (Figures \ref{fig:moons_grid_relu} to \ref{fig:cifar10_grid}), we have found evidence that there exists a minimum width required to solve a problem. In our piece-wise linear approximation to the relation between width and depth (i.e. Equation \ref{eq:dependencyWidthDepth}), such parameter is $\hat{W_{min}}$. Indeed, in our experiments, we see that for widths below $\hat{W}_{min}$ resulting networks lack the capacity to solve the problem (i.e. reach a certain accuracy values). We venture to suggest this threshold is \emph{hard}, and that it is related to geometric properties of the dataset. 
\\\\
However, as networks are made deeper, they start failing due to a variety of reasons (we dare say, mainly due to dead units/points). In this sense,  after a certain depth $\hat{D}$ it is no longer possible train networks to effectively solve the problem using this minimum width $\hat{W_{min}}$. As a consequence, we must model this depth-width relation as a piece-wise function that is on one side constant, and we dare say \emph{linear} on the other (agian, recall Equation \ref{eq:dependencyWidthDepth}). Further inquiry is needed to verify such finding. Regardless, our experimentation shows that using \SepUnitPoint enables us to train twice as deep networks than \ReLUBN without running into this problem, see Table \ref{tab:dependency}. 
\\\\
If we define the quantity  $\Delta{W} = W_{min}(D) - \hat{W}$ whenever $D > \hat{D}$ we conjecture that there are $\Delta{W}$ units that do not necessarily \emph{solve} the problem, but rather enable back-propagation. In addition, if $D<\hat{D}$, the network will reach maximum accuracy if we choose such that $\Delta{W}>0$. Thus, The rationale behind the width-increase strategy, is to increase layer width to surpass $\W_{min}(D)$, or equivalently add new hyper-planes in order to avoid dead points, based on empirical evidence (namely our grids like \ref{fig:moons_grid_relu}, for \ReLU). This black region (of trivial accuracy) seems unavoidable. However, further inquiry is required to established precisely a connection between layer-width (i.e. number of hyperplanes) and dead points, perhaps in connection to dataset geometry. 
\\\\


\\\\
Under our conjecture, we interpret also these additional units as necessary to increase the chances of finding the set of units of size $W_{min}(D)$ that allows maximal accuracy from the start. We call this proverbial set of units, the \emph{winning lottery ticket} as described in \cite{lottery}. However, further inquiry into the role of loss and back-propagation (featuring the interaction between constraint loss and main loss) is needed to establish whether this winning lottery ticket is stable. For the moment, the only interaction between constraint and main loss is encoded in parameter $\lambda$. 
\\\\
Upon revision of the loss plots in our experiments, and resulting separation plots we find several instabilities and behaviours while on training, from which interesting properties are in need of further research. In addition,  the use of the separation constraints in other types of networks such as LSTM \cite{lstm} or transformers \cite{transformer}\cite{transformer2} is yet to be performed, to explore other tasks like regression, unsupervised learning \cite{embedding}, graphs \cite{graph} or generation \cite{gan,vae}. It is important to test the \SepConstraint family on more challenging datasets.  Alternatively, while the separation constraints prevent the vanishing gradient, the \emph{exploding gradient} remains at large. While the \SepConstraint family places \emph{lower bounds} on the magnitude of the pre-activation values, it does not place upper bounds, for which the possibility opens to define other types of constraints. 





