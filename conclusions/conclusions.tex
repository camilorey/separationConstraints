\section{Conclusions}\label{sec:conclusions}
Through the \SepConstraint family, we have shown that deeper networks can be trained without resorting to any architectural modifications or data manipulations. Moreover, it can be done to surpass Batch Normalization. The lesson to be learned here is that the training of deeper networks can be achieved \emph{accomodating} the network for data, rather than manipulating it.
\\\\
We find great value in Equation \ref{eq:dependencyWidthDepth}, as a conjecture over the relation between width and depth. In our proposal, $\hat{D}$ plays a fundamental role in the development of maximal accuracy: (1) for depths lower than $\hat{D}$, the minimum width required to reach maximal accuracy is exactly $\hat{W}_{min}$, in the sense that choosing $W\geq\hat{W}_{min}$ assures maximal accuracy; (2) meanwhile when  $D\geq \hat{D}$, choosing $W\geq W_{min}(D)$, ensures maximal accuracy. However, our results are limited only to educated guesses based on empirical evidence. Further study is required both to verify/refine our width-depth relation (particularly function $h(D)$ in Equation \ref{eq:dependencyWidthDepth}), and to establish its relation to dataset geometry, architecture and others. As shown in Table \ref{tab:dependency}, \SepUnitPoint displays a $\hat{D}$ more than twice the value of \ReLUBN. In turn, we can train networks more than twice as deep with the same width than \ReLUBN. 
\\\\
Despite the fact that no information is provided in this article about dynamics of training using the \SepConstraint formulation, preliminary revision, shows interesting dynamic behavior on behalf of the \SepConstraint family, particularly regarding instabilities that appear during training. In addition, the use of the separation constraints in other types of networks such as \texttt{LSTM} \cite{lstm} or transformers \cite{transformer}\cite{transformer2} is yet to be performed, to explore other tasks like regression, unsupervised learning \cite{embedding}, graphs \cite{graph} or generation \cite{gan,vae}. Alternatively, while the separation constraints prevent the vanishing gradient, the \emph{exploding gradient} remains at large. While the \SepConstraint family places \emph{lower bounds} on the magnitude of the pre-activation values, it does not place upper bounds, for which the possibility opens to define other types of constraints. 
\\\\






