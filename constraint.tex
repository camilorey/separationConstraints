
\section{Separating constraints}
If we regard a DNN as a function in the form of equation \ref{eq:network}. such network depends on a collection $\theta$ of parameters organized as follows. 
\\\\
For a layer $\layer_k:\Real{n}\rightarrow\Real{m}$, we denote its collection of parameters $\theta_k$ of the form
\begin{equation}
    \theta_k = \{(\vec{w}^k_j,b^k_j)|j=1,\ldots,m\}
\end{equation}
thus, $\theta$ is composed of the union of the parameters of each layer:
\begin{equation}
    \theta = \displaystyle\bigcup_{k=1}^D\theta_k
\end{equation}
thus, given $F$ as a function depends on both its input $\vec{x}$. The parameters in collection $\theta$ are set during the training process via the minimization of some loss functional. That is we seek the solution of the following equation
\begin{equation}\label{eq:generalOptimizationProblem}
\argmin_{\theta} \mathcal{L}(\mathcal{T},\theta)
\end{equation}
for some loss functional that depends on the training set $T$ and $\theta$ (see for example \cite{LeCun06atutorial} for available choices in classification problems). 
\\\\
Conceptually, we wish to enforce \ReLU \emph{separability} based on the definition provided in equation \ref{eq:separabilityDefinition}. However, set theoretic functions are not differentiable \cite{Glorot10Initialization,lecun2015DeepLearningBig,munkres2000Topology}. 
\\\\
To bridge that gap, we will make an adaptation based on  the technique of Support Vector Machines as presented for example in  \cite{Burges1998TutorialOnSVMForPatternRecognition} or \cite{Hearst1998SupportVectorMachines}. 
\\\\\
That is, given a fixed choice of weight vector  $\vec{w}\in\Real{n}$ and bias $b\in\mathbb{R}$, we define \emph{margins} $\xi^{+},\xi^{-}\geq 0$ for a dataset $\mathcal{T}$ (via set $X$) as a pair of numbers such that
\begin{equation}\label{eq:basicConstraintFormulation}
\begin{array}{lcl}
    \max\{\vec{w}\cdot\vec{x}+b|\vec{x}\in X\}\geq 1-\xi^{+}\\
    \min\{\vec{w}\cdot\vec{x}+b|\vec{x}\in X\}\leq -1+\xi^{-}\\
\end{array}
\end{equation}
This margin parameters can be added to the original optimization problem  (equation \ref{eq:generalOptimizationProblem}) as
\begin{equation}\label{eq:generalOptimizationProblem}
\argmin_{\theta,\xi^{+},\xi^{-}} \mathcal{L}(\mathcal{T},\theta) + \lambda g(\xi^{+},\xi^{-})
\end{equation}
where $\lambda$ is a \emph{hyper-parameter} that introduces a trade-off between the loss fulfilling of the constraints. Intuitively, we mean to \emph{balance} the effect of the constraints within the semantics of the problem.
Also, we will require that function $g$ is non-negative, so that in the limit (i.e. when minimized), the original loss will keep the training process going. 
\\\\
The intuition behind the introduction of these constraints presented in Equation \ref{eq:basicConstraintFormulation} is as follows. 
\\\\
In the same spirit of SVM, the $\xi^{+}$ intends to create pre-activation values less (or equal) than 1, in order to  hopefully force the weights and biases to be chosen so that the lower part of the units is non-empty. 
\\\\
In symmetry, enforcing the constraint with $\xi^{-}$ promotes pre-activation of points of $X$ near zero. This intends to ensure that the upper parts of the units are non-empty. 
\\\\
Thus, since neither the upper, nor the lower part of the unit is empty (i.e. the units are neither dead nor redundant in the sense of equations \ref{eq:deadNeuronVersion1} and equation \ref{eq:redundantNeuron} respectively).
\\\\
However, fixing a hard margin as done in SVM may yield an unfeasible problem. In that sense, both $\xi^{+}$ and $\xi^{-}$ work as \emph{slack} variables (a soft margin) \cite{Burges1998TutorialOnSVMForPatternRecognition,Hearst1998SupportVectorMachines}.
\\\\
As an improvement over preceding techniques, and in order to provide some heuristic not to deadlock the training process with the introduction of function $g$ in the loss funtional, we suggest that we start with \emph{all} parameters to zero (i.e. perform a \emph{zero-initialization}). 
\\\\
Moreover, we introduce a basic template for function $g$:
\begin{equation}\label{eq:definitionOfRho}
    g(\xi^{+},\xi^{-}) = \rho\xi^{+}+(1-\rho)\xi^{-}
\end{equation}
Intuitively this choice of function $g$ has two meanings. On the one hand, we choose to minimize a weakened version of the $L_1$ norm of the vector $\vec{xi}=(\xi^{+},\xi^{-})$ to minimize the margin values. On the other, the choice of $\rho\neq 1/2$ will force the separation to be non-symmetrical. We belive that this will force the gradient of the loss (augmented with $g$) to induce a richer dynamic while under training \cite{LeCun06atutorial}. 
\\\\
What remains of this section is to choose margins and enforce constraints at different structures of a network: by unit, by layer, and \emph{by point}.

\subsection{Unit based separation constraints \SepUnit}
Within a \ReLU DNN $F$, choose a unit $u_j^k$ in layer $\layer_k:\Real{n_k}\rightarrow\Real{m_k}$. This unit will be defined univocally by its parameters:
\begin{equation}\label{eq:unitSepParameterWriting}
    u^k_j(\vec{x}) = u(\vec{x};\vec{w}^k_j,b^k_j)
\end{equation}
in order to enforce separation within this unit, we must substitute parameters in equation \ref{eq:basicConstraintFormulation} and create constraints of the form:
\begin{equation}
    \begin{array}{lcl}
    \max\{\vec{w}^k_j\cdot\vec{x}+b^k_j|\vec{x}\in X\}\geq 1-\xi^{+}_{jk}\\
    \min\{\vec{w}^k_j\cdot\vec{x}+b^k_j|\vec{x}\in X\}\leq -1+\xi^{-}_{jk}\\
\end{array}
\end{equation}
for $k=1,\ldots,D$ and $j$ the number of units per layer. As a consequence, we will need to introducuce balancing parameters $rho_{jk}$ on function $g$. Individually, we will need to minimize functions of the form
\begin{equation}
    g(\xi^{+},\xi^{-}) = \rho_{jk}\xi^{+}_{jk}+(1-\rho_{jk})\xi^{-}_{jk}
\end{equation}
so that at a global scale we will have to add to the loss functional a term of the form:
\begin{equation}\label{eq:constraintLossForUnitSeparation}
    G = \sum_{k=1}^{D}\sum_{j=1}^{m_k}\rho_{jk}\xi^{+}_{jk}+(1-\rho_{jk})\xi^{-}_{jk}
\end{equation}
We name this the \emph{separation by unit} formulation \SepUnit.

\subsubsection{Layer Based Separation \SepLayer}

As previous network architectures like \texttt{ResNet} and \texttt{DenseNet} have shown us, it is desirable sometimes to \emph{skip} connections in between layers, for \emph{accuracy} can improve linearly dependent on depth. However, this growth in accuracy is not unbounded (CITA POR FAVOR). 
\\\\
Thus, it is important to provide a mechanism to forward the output of intermediate layers to the top of the network to which our constraint formulation can adapt and coalesce with existing techniques.  
\\\\
We suggest to relax the \SepUnit formulation for the sake of computational complexity and provide the reader with the \SepLayer formulation. The intuition here is to require that at least \emph{one} of the pre-activations satisfies a soft SVM margin (equation  \ref{eq:basicConstraintFormulation}), per layer.  
\\\\
With this change we allow the presence of redundant units, but ensure that at least one of them will be separating. That is, given a layer $\layer_k$ we define the \emph{layer} margins $\xi^{-}_k$ and $\xi^{+}_k$, and rewrite equation \ref{eq:basicConstraintFormulation} as follows:
\begin{equation}\label{eq:layerSeparationConstraint}
\begin{array}{lcl}
    \displaystyle\max_{\vec{x}\in{X}}\max_{j=1,\ldots,m_k}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\geq 1-\xi^{+}_k\\\\
    \displaystyle\min_{\vec{x}\in{X}}\min_{j=1,\ldots,m_k}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\leq -1+\xi^{-}_k\\
\end{array}    
\end{equation}
We will need to introduce only $D$ constraints (for each layer). In addition, the term to minimize alongside with the loss functional is as follows:
\begin{equation}\label{eq:constraintLossForLayerSeparation}
    G = \sum_{k=1}^{D}\lambda_k(\rho_{k}\xi^{+}_{k}+(1-\rho_{k})\xi^{-}_{k})
\end{equation}
we call this the \SepLayer formulation. Notice that we have introduced additional hyperparameters $\lambda_k$ to focus on the role specific layers.   

\subsection{Point Based Separation \SepPoint}

In practice, we do not use the entire dataset when under training, but rather use \emph{batches} and use the stochastic \emph{subgradient} during the training process \cite{LeCun06atutorial}. However, small batch size may cause inconsistencies within the training process (due for example to points that are very far apart). 
\\\\
We need to make our constraint formulation impervious to batch choice by focusing on the values that constraints take on specific points. 
\\\\
That is, we will constraint the slacks $\xi^{\pm}$ to specific points on the batch, so that at least one of them satisfies the constraints. That is given $\vec{x}\in X$, and $u_1,\ldots,u_m$ unit functions in a layer, we define slack variables $\xi^{-}_{xk},\xi^{+}_{xk}\geq 0$ in the context of the following constraints:
\begin{equation}\label{eq:pointSeparationConstraint}
\begin{array}{lcl}
    \displaystyle\max_{j=1,\ldots,m}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\geq 1-\xi^{+}_{xk}\\\\
    \displaystyle\min_{j=1,\ldots,m_k}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\leq -1+\xi^{-}_{xk}\\
\end{array}    
\end{equation}
so that the associated term to minimize will have the following form:
\begin{equation}\label{eq:constraintLossForLayerSeparation}
    G = \sum_{k=1}^{D}\rho_{k}\xi^{+}_{xk}+(1-\rho_{k})\xi^{-}_{xk}
\end{equation}
