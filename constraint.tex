
\section{Separating constraints}\label{sec:constraint}
If we regard a DNN as a function in the form of equation \ref{eq:network}. such network depends on a collection $\theta$ of parameters organized as follows. 
\\\\
For a layer $\layer_k:\Real{n}\rightarrow\Real{m}$, we denote its collection of parameters $\theta_k$ of the form
\begin{equation}
    \theta_k = \{(\vec{w}^k_j,b^k_j)|j=1,\ldots,m\}
\end{equation}
thus, $\theta$ is composed of the union of the parameters of each layer:
\begin{equation}
    \theta = \displaystyle\bigcup_{k=1}^D\theta_k
\end{equation}
thus, given $F$ as a function depends on both its input $\vec{x}$. The parameters in collection $\theta$ are set during the training process via the minimization of some loss functional. That is we seek the solution of the following equation
\begin{equation}\label{eq:generalOptimizationProblem}
\argmin_{\theta} \mathcal{L}(\mathcal{T},\theta)
\end{equation}
for some loss functional that depends on the training set $T$ and $\theta$ (see for example \cite{LeCun06atutorial} for available choices in classification problems). 
\\\\
Conceptually, we wish to enforce \ReLU \emph{separability} based on the definition provided in equation \ref{eq:separabilityDefinition}. However, set theoretic functions are not differentiable \cite{Glorot10Initialization,lecun2015DeepLearningBig,munkres2000Topology}. 
\\\\
To bridge that gap, we will make an adaptation based on the technique of Support Vector Machines as presented for example in  \cite{Burges1998TutorialOnSVMForPatternRecognition} or \cite{Hearst1998SupportVectorMachines}. 
\\\\\
That is, given a fixed choice of weight vector  $\vec{w}\in\Real{n}$ and bias $b\in\mathbb{R}$, we define \emph{margins} $\xi^{+},\xi^{-}\geq 0$ for a dataset $\mathcal{T}$ (via set $X$) as a pair of numbers such that
\begin{equation}\label{eq:basicConstraintFormulation}
\begin{array}{lcl}
    \max\{\vec{w}\cdot\vec{x}+b|\vec{x}\in X\}\geq 1-\xi^{+}\\
    \min\{\vec{w}\cdot\vec{x}+b|\vec{x}\in X\}\leq -1+\xi^{-}\\
\end{array}
\end{equation}
This margin parameters can be added to the original optimization problem  (equation \ref{eq:marginOptimizationProblem}) as
\begin{equation}\label{eq:marginOptimizationProblem}
\argmin_{\theta,\xi^{+},\xi^{-}} \mathcal{L}(\mathcal{T},\theta) + \lambda g(\xi^{+},\xi^{-})
\end{equation}
where $\lambda$ is a \emph{hyper-parameter} that introduces a trade-off between the loss fulfilling of the constraints, a technique known as \emph{scalarization} \cite{boyd}. Intuitively, we mean to \emph{balance} the effect of the constraints within the semantics of the problem.
Also, we will require that function $g$ is non-negative, so that in the limit (i.e. when minimized), the original loss will keep the training process going. 
\\\\
The intuition behind the introduction of these constraints presented in Equation \ref{eq:basicConstraintFormulation} is as follows. 
\\\\
In the same spirit of SVM, the $\xi^{+}$ intends to create AT LEAST ONE pre-activation value GREATER (or equal) than 1, in order to  hopefully force the weights and biases to be chosen so that the lower part of the units is non-empty. 
\\\\
In symmetry, enforcing the constraint with $\xi^{-}$ promotes AT LEAST ONE pre-activation of points of $X$ BELOW -1. This intends to ensure that the upper parts of the units are non-empty. 
\\\\
Thus, since neither the upper, nor the lower part of the unit is empty (i.e. the units are neither dead nor redundant in the sense of equations \ref{eq:deadNeuronVersion1} and equation \ref{eq:redundantNeuron} respectively).
\\\\
NO SE SI HACE FALTA However, fixing a hard margin as done in SVM may yield an unfeasible problem. In that sense, both $\xi^{+}$ and $\xi^{-}$ work as \emph{slack} variables (a soft margin) \cite{Burges1998TutorialOnSVMForPatternRecognition,Hearst1998SupportVectorMachines}.
\\\\
VER SUBSECTION PARA ELLO As an improvement over preceding techniques, and in order to provide some heuristic not to deadlock the training process with the introduction of function $g$ in the loss funtional, we suggest that we start with \emph{all} parameters to zero (i.e. perform a \emph{zero-initialization}). 
\\\\
Moreover, we introduce a basic template for function $g$:
\begin{equation}\label{eq:definitionOfRho}
    g(\xi^{+},\xi^{-}) = \rho\xi^{+}+(1-\rho)\xi^{-}
\end{equation}

EXPLICAR QUE RHO ES PARA ZERO INIT

In order to enable zero initialization we have the problem of that the gradient of the positive and negative part of the constraint cancels each other. In order to solve this problem, we introduce a convex combination of the constraints which weights each one. With this we can bias them to the positive side with an small value (0.51 in practice). Doing so to the negative would send all the data to the negative, thus killing the unit MIRAR ESTO. Once the first iteration has passed this we deem the impact of the imbalance negligible.

Intuitively this choice of function $g$ has two meanings. On the one hand, we choose to minimize a weakened version of the $L_1$ norm of the vector $\vec{xi}=(\xi^{+},\xi^{-})$ to minimize the margin values. On the other, the choice of $\rho\neq 1/2$ will force the separation to be non-symmetrical. We believe that this will force the gradient of the loss (augmented with $g$) to induce a richer dynamic while under training \cite{LeCun06atutorial}. 
\\\\
What remains of this section is to choose margins and enforce constraints at different structures of a network: by unit, by layer, and \emph{by point}.

\subsection{Unit based separation constraints \SepUnit}\label{subsec:sepUnit}
Within a \ReLU DNN $F$, choose a unit $u_j^k$ in layer $\layer_k:\Real{n_k}\rightarrow\Real{m_k}$. This unit will be defined univocally by its parameters:
\begin{equation}\label{eq:unitSepParameterWriting}
    u^k_j(\vec{x}) = u(\vec{x};\vec{w}^k_j,b^k_j)
\end{equation}
in order to enforce separation within this unit, we must substitute parameters in equation \ref{eq:basicConstraintFormulation} and create constraints of the form:
\begin{equation}
    \begin{array}{lcl}
    \max\{\vec{w}^k_j\cdot\vec{x}+b^k_j|\vec{x}\in X\}\geq 1-\xi^{+}_{jk}\\
    \min\{\vec{w}^k_j\cdot\vec{x}+b^k_j|\vec{x}\in X\}\leq -1+\xi^{-}_{jk}\\
\end{array}
\end{equation}
for $k=1,\ldots,D$ and $j$ the number of units per layer. As a consequence, we will need to introducuce balancing parameters $rho_{jk}$ on function $g$. Individually, we will need to minimize functions of the form
\begin{equation}
    g(\xi^{+},\xi^{-}) = \rho_{jk}\xi^{+}_{jk}+(1-\rho_{jk})\xi^{-}_{jk}
\end{equation}
so that at a global scale we will have to add to the loss functional a term of the form:
\begin{equation}\label{eq:constraintLossForUnitSeparation}
    G = \sum_{k=1}^{D}\sum_{j=1}^{m_k}\rho_{jk}\xi^{+}_{jk}+(1-\rho_{jk})\xi^{-}_{jk}
\end{equation}
We name this the \emph{separation by unit} formulation \SepUnit.

However, this formulation lacks for a simple reason. Although we are forcing the units to not to be dead or redundant, several configurations are still possible which would harm the performance in a trivial way, for instance, we can set a point to have a pre-activation greater than 1 for all the units of the layer and the rest to zero. Having most of the dataset sent to zero is obviously a bad idea, see Eq. \ref{eq:unitFail}

\begin{equation}\label{eq:unitFail}
    \bordermatrix{ &u_1& u_2&\ldots &u_l\cr
                x_1& 1 &  1  & \ldots & 1\cr
                x_2& 0  &  0 & \ldots & 0\cr
                x_i& \vdots & \vdots & \ddots & \vdots\cr
                x_n& 0  &   0       &\ldots & 0} \\[15pt]
\end{equation}


\subsection{Point Based Separation \SepPoint}\label{subsec:sepPoint}

In order to fix the trivial failure modes with \SepUnit describe at \ref{subsec:sepUnit}, where several points have null activation, we introduce a constraint focused on points. Additionally, as in practice we do not use the entire dataset when under training, but rather use \emph{batches} \cite{LeCun06atutorial}, small batch size may cause inconsistencies within the training process (lack of commesurality of the batch statistics with the rest of the dataset, excessive locality of the sample might lead to lack of generalization and convergence, etc...). In our case, those issues would translate in penalizing the units by not fulfilling \SepUnit in the current batch because the point that makes them comply is in another batch.  
\\\\
We need to make our constraint formulation impervious to batch choice by focusing on the values that constraints take on specific points as well as fixing trivial failure modes. 
\\\\
That is, we will constraint the slacks $\xi^{\pm}$ to specific EACH? points on the batch, so that at least one of them satisfies the constraints. That is given $\vec{x}\in X$, and $u_1,\ldots,u_m$ unit functions in a layer, we define slack variables $\xi^{-}_{xk},\xi^{+}_{xk}\geq 0$ in the context of the following constraints:
\begin{equation}\label{eq:pointSeparationConstraint}
\begin{array}{lcl}
    \displaystyle\max_{j=1,\ldots,m}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\geq 1-\xi^{+}_{xk}\\\\
    \displaystyle\min_{j=1,\ldots,m_k}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\leq -1+\xi^{-}_{xk}\\
\end{array}    
\end{equation}
so that the associated term to minimize will have the following form \ref{eq:convexCombinationOfConstraints}:
\begin{equation}\label{eq:convexCombinationOfConstraints}
    G = \sum_{k=1}^{D}\rho_{k}\xi^{+}_{xk}+(1-\rho_{k})\xi^{-}_{xk}
\end{equation}

This forces that each point has a positive and negative pre-activation, thus solving the problem of \SepUnit where there were points which never/always activated. However, we remark that as this constraint does not place any restrictions to the units in regard on whether they are affine or not, a linear separation is perfectly valid, so often the use of this constraint alone results in solutions close to the linear form. Also, it is possible that one unit is activated for all the points and the rest are dead, as we show in eq. \ref{eq:pointFail}.

\begin{equation}\label{eq:pointFail}
    \bordermatrix{ &u_1& u_2&\ldots &u_l\cr
                x_1& 1 &  0  & \ldots & 0\cr
                x_2& 1  &  0 & \ldots & 0\cr
                x_i& \vdots & \vdots & \ddots & \vdots\cr
                x_n& 1  &   0       &\ldots & 0} \\[15pt]
\end{equation}

\subsection{Unit and Point Based Separation \SepUnitPoint}\label{subsec:sepUnitPoint}

In order to overcome the problems previously exposed in \ref{subsec:sepUnit} and \ref{subsec:sepPoint}, we combine them in which we call \SepUnitPoint. With this we find experimentally that both problems, linearity of \SepPoint and trivial failure of \SepUnit are solved.

\subsection{Layer Based Separation \SepLayer}\label{subsec:sepLayer}

As previous network architectures like \texttt{ResNet} and \texttt{DenseNet} have shown us, it is desirable sometimes to \emph{skip} connections in between layers, for \emph{accuracy} can improve dependent on depth \cite{resnet, densenet}. However, this growth in accuracy is not unbounded and past some point it degrades \cite{simpnet}. 
\\\\
Thus, it is important to provide a mechanism to forward the output of intermediate layers to the top of the network to which our constraint formulation can adapt and coalesce with existing techniques. Additionally, the number of constraints that we need to place, especially in \SepUnitPoint, is large and we would like to find a way to reduce the computational complexity.
\\\\
We suggest to relax the \SepUnit formulation for that aim and provide the reader with the \SepLayer formulation. The intuition here is to require that at least \emph{one} of the pre-activations of the entire batch is greater than 1 and another is less than -1, per layer.  
\\\\
With this change we allow the presence of redundant units, but ensure that at least for all the points in the batch and all the pre-activations of the layer, there is at least one pre-activation greater (or lesser) than 1 (-1). That is, given a layer $\layer_k$ we define the \emph{layer} margins $\xi^{-}_k$ and $\xi^{+}_k$, and rewrite equation \ref{eq:basicConstraintFormulation} as follows:
\begin{equation}\label{eq:layerSeparationConstraint}
\begin{array}{lcl}
    \displaystyle\max_{\vec{x}\in{X}}\max_{j=1,\ldots,m_k}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\geq 1-\xi^{+}_k\\\\
    \displaystyle\min_{\vec{x}\in{X}}\min_{j=1,\ldots,m_k}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\leq -1+\xi^{-}_k\\
\end{array}    
\end{equation}
We will need to introduce only $D$ constraints (for each layer). In addition, the term to minimize alongside with the loss functional is as follows:
\begin{equation}\label{eq:constraintLossForLayerSeparation}
    G = \sum_{k=1}^{D}\lambda_k(\rho_{k}\xi^{+}_{k}+(1-\rho_{k})\xi^{-}_{k})
\end{equation}
we call this the \SepLayer formulation. Notice that we have introduced additional hyperparameters $\lambda_k$ to focus on the role specific layers.   


