\section{Neural networks and unit separability}\label{sec:separability}

A traditional DNN \emph{neural network} is composed of \emph{layers} whose input is the output of the previous \emph{layer} \ref{eq:layer}, which are composed of a set of arbitrary real-valued functions in $\mathbb{R}$ called \emph{units}\ref{eq:unit}. The units are commonly composed of an affine transform, parameterized by a vector of weights $\vec{w}$ and an scalar bias $b$, followed by a non-linear \emph{activation function} $\sigma : \mathbb{R}\rightarrow\mathbb{R}$. In the case of the \ReLU this activation function is consist in the truncation to zero of negative values.

\begin{equation}\label{eq:unit}
u(x; \vec{w}, b) = \sigma(\vec{w} \cdot \vec{x} + b)
\end{equation}
Given a collection $\vec{w}_1\ldots,\vec{w}_m\in\Real{m}$ of weight vectors and $b_1,\ldots,b_m\in\mathbb{R}$ a collection of bias, we simplify the notation via
\begin{equation}
    u_j(\vec{x}) = u(\vec{x};\vec{w}_j,b_j)
\end{equation}
for $j=1,\ldots,m$. Units can be linearly combined with canonical vectors of $\Real{M}$ to construct \emph{layer} functions $\layer:\Real{N}\rightarrow\Real{M}$ where $N$ is the layer \emph{dimension} and the number $M$ is the \emph{width} of the layer.

\begin{equation}\label{eq:layer}
\layer(\vec{x}) = \sum^M_{i=1} u_i(\vec{x}) \hat{\textbf{e}}_i
\end{equation}
on more plain terms, $\layer(\vec{x})$ is an $m$-tuple (a \emph{concatenation}) of the outputs of units $u_1,u_2,\ldots,u_m$.
\\\\
A DNN (a neural network) is a function $F:\Real{N_0}\rightarrow\mathbb{R}$ defined by a finite collection of size $L$ of layer functions which map the data points to the targets through function composition, by mapping a into a series of \emph{intermediate representations} \ref{eq:network}. \cite{ramachandranEtAl2017SearchingForActivationFunctions,eswaranSingh2015SomeTheoremsForFFNN}.
\begin{equation}\label{eq:network}
F(x) = \layer_D \cdot \layer_{D-1} \dots \layer_1(x) 
\end{equation}
Now we want to relate the network to the data provided for solving the task. That is, \emph{formally define} the \emph{dataset}. 
\\\\
Given a collection $X=\{\vec{x}_1,\vec{x}_2,\cdots,\vec{x}_N\} \subset \Real{N}$ of points and their corresponding target values $Y=\{\vec{y}_1,\vec{y}_2,\cdots,\vec{y}_N,\} \subset \Real{M}$, we define a dataset as a pairing \ref{eq:dataset}.

\begin{equation}\label{eq:dataset}
    \mathcal{T} = \{(\vec{x_p}, \vec{y_p}) | p=1,\cdots,K\}
\end{equation}

Equation \ref{eq:unit} enables us to define two different sets on the data with regards to their pre-activation.  
\\\\
We define the \emph{upper} part of an unit  $u = u(\cdot;\vec{w},b)$ as the set of points of the \emph{dataset} for which the pre-activation is positive:
\begin{equation}\label{eq:upperPartOfUnit}
    upper(u) = \{\vec{x}|\vec{w}\cdot\vec{x}+b>0\}
\end{equation}
and the \emph{lower} part of $u_j$ as the set of points for which the pre-activation of $u_j$ turns up a negative value. That is, 
\begin{equation}\label{eq:lowerPartOfUnit}
    lower(u) = \{\vec{x}|\vec{w}\cdot\vec{x}+b\leq 0\}
\end{equation}
Note that both upper and lower parts are --in the sense of linear algebra-- \emph{affine spaces} of $\Real{n}$\cite{Burges1998TutorialOnSVMForPatternRecognition,florenzano2001ConvexAnalysis}. 
\\\\
In addition, Their boundary is defined by the hyperplane with normal vector $\vec{w}$ \emph{translated} by the bias $b$ \cite{boyd,florenzano2001ConvexAnalysis,Burges1998TutorialOnSVMForPatternRecognition}.

\subsection{\ReLU separability}\label{subsec:ReLUSeparability}

Though \emph{separability} is an intrinsic property of real-valued functions (see for example the \emph{separation theorems} of convex analysis in \cite{florenzano2001ConvexAnalysis}  or \cite{Burges1998TutorialOnSVMForPatternRecognition}), we are interested in studying separability with regards to specific data sets $\mathcal{T}$. 
\\\\
In order to do so, we isolate all the first components of points in $\mathcal{T}$ in a set $X$. That is, 
\begin{equation}
    X = \{\vec{x}_i|i=1,\ldots,P\}
\end{equation}
for all the pairs in $\mathcal{T}$ (in the sense of equation \ref{eq:dataset}). 
\\\\
Thus, we can define \emph{unit separability} for a given dataset $\mathcal{T}$ via $X$. More specifically, we say that a unit $u:\Real{n}\rightarrow\mathbb{R}$ is \emph{dead} with regards to set $X$ if
\begin{equation}\label{eq:deadUnitVersion1}
 upper(u)\cap X = \emptyset 
\end{equation}
this means that unit $u_j$ is not activates by any point in the data set. On symmetry  we say that $u$ is \emph{affine} if it is activated by \emph{every} point in $X$. That is, 
\begin{equation}\label{eq:affineUnit}
 upper(u)\cap X = X
\end{equation}
notice that this characterization is also valid using the lower part of $u$. Namely, $u$ is dead with regards to set $X$ if
\begin{equation}\label{eq:deadNeuronVersion2}
    lower(u)\cap X = X
\end{equation}
and $u$ is also affine in terms of the lower part of $u$ if 
\begin{equation}
    lower(u)\cap X = \emptyset
\end{equation}


