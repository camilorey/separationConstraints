\section{Introduction}\label{sec:introduction}
The success of Deep Learning has been linked to its ability to learn \emph{abstract representations} from input data in a hierarchical fashion  \cite{LeCun06atutorial,ramachandranEtAl2017SearchingForActivationFunctions}, using Deep Neural Networks (DNN for short). However, depth is also instrumental in various DNN \emph{issues} like the \emph{vanishing gradient} as presented in \cite{vanishing1} or \cite{vanishing2}; the \emph{exploding gradient} as presented in \cite{exploding}; the \emph{dead unit problem} as presented in \cite{leaky},\cite{whyreludie} or \cite{whenneuronsfail}; or the \emph{degradation problem} as presented in \cite{resnet}. 
\\\\
% Problema general
We claim we can categorize existing methods to address these issues in two families: \emph{architectural modifications} to the DNN and input data \emph{manipulation}. Examples of architectural modifications include (1) layer-width increase as done in \cite{wideresnet,inceptionv1}; (2) feed-forward structure network modification (as done in ResNets \cite{resnet} or DenseNets \cite{densenet}); (3) unit based alterations (as done in \emph{leaky}-\ReLU \cite{leaky} or \texttt{PReLU} \cite{prelu}). Meanwhile, the most commonly used data manipulation technique is \emph{data normalization} on the output of layers as done under \emph{batch normalization} \cite{batchnorm}.  
\\\\
%occur amidst several geometric undertow \todo{we claim that these issues ocurr at geometric level, have a geometric background/interpretation}, that when unamanged, corrupt \todo{hinder} back-propagation. 

In the case of \ReLU based DNN, a geometric outlook allows us to conjecture on these issues differently. Under the setting defined by Rey-Torres et al. in \cite{reyRiera2019ModellingClassificationReLU},  gradient issues (vanishing and exploding) depend on \emph{dying-items} (e.g. dead units or points). In turn, dying items can be defined by  \emph{incorrect} positioning of the hyper-planes from the pre-activation of \ReLU units (\emph{relative} to the dataset). The reader can review our presentation of our rationale in Section \ref{sec:separability} on \emph{Separability}. 
\\\\
Thus, we can correct plane positioning by enforcing \emph{separation constraints} as detailed in Section \ref{sec:constraint}) titled \emph{Separation Constraints}. Indeed, by targeting specifically the dead unit problem (defining the unit-based separation constraint \SepUnit); and the dead point problem (using a point-based separation constraint \SepPoint). Intuitively, \SepUnit constraints aim to secure unit pre-activation hyperplanes that \emph{cut} throughout the dataset (as detailed in Section \ref{sec:separability}). Meanwhile,  \SepPoint aims to secure the existence of units with opposing pre-activation values at each dataset point. 
\\\\
We provide experimentation showing how using separation constraints allows to train deeper and thinner networks, reducing the dependence between width and depth introduced in \cite{simpnet} and \cite{densenet} Section \ref{sec:experiments}. Our insights on the mechanics and dynamics \ReLU-based DNN separation, are detailed throughout our Final Remarks (Section \ref{sec:conclusions}).  

\begin{table*}[h!]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Family & Method & Examples & Intended issues\\ 
\midrule
%\multirow{3}{*}
Architectural & Width increase & \begin{tabular}[c]{@{}l@{}}
 Wide \texttt{ResNet} \cite{wideresnet}\\ 
\texttt{Inception} \cite{inceptionv1}\\
\end{tabular}                                                                 
& \begin{tabular}[c]{@{}l@{}}
Vanishing Gradient\\ 
Train deeper networks
\end{tabular}    \\
& 
\begin{tabular}[c]{@{}l@{}}
Connection \\ 
modification
\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}
 \texttt{ResNet} \cite{resnet} \\
 \texttt{DenseNet}\cite{densenet}\\
\end{tabular}                                   
& \begin{tabular}[c]{@{}l@{}}
Vanishing Gradient\\ 
\\ 
Increase Depth\end{tabular} 
\\
& Unit Alteration                                                    
& \begin{tabular}[c]{@{}l@{}}
\emph{leaky}-\ReLU \cite{leaky} \\ 
\texttt{PReLU} \cite{prelu}\\ 
\texttt{C-ReLU} \cite{crelu}
\end{tabular} & \begin{tabular}[c]{@{}l@{}}
Vanishing Gradient\\ 
Dead Neurons
\end{tabular} \\
Data Manipulation & Modify layer output & \emph{batch normalization} \cite{batchnorm}  & Exploding Gradient                  \\ 
\bottomrule
\end{tabular}
\caption{Summary of techniques commonly used in deep learning to overcome issues like enhanced depth, vanishing/exploding gradient and dead neurons.}
\label{tab:techniquesTable}
\end{table*}

