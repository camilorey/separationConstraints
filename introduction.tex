\begin{abstract}
In Deep Learning, thin neural networks networks are especially prone to issues like dying neurons, overfitting, vanishing and exploding gradients. When using \ReLU architectures a series of linear separability  constraints  can  be  introduced  in order to (1)  allow  us  to  initialize  the  weights  to  zero, (2) control the aforementioned issues to a degree comparable to Batch Normalization; and (3)  enhance our geometric insight and construct interpretations of the dynamic behavior of DNN using elementary linear algebra.
\end{abstract}

\section{Introduction}\label{sec:introduction}

% Párrafo introductorio
The success of Deep Learning has traditionally been linked to its ability to learn \emph{abstract representations} of data using function composition\cite{LeCun06atutorial}. These functions (i.e. the \emph{layers}) decompose and solve the problem in \emph{parts} \cite{resnetSubtree} profiting their feed-forward structure \cite{backprop}. However, the deeper the network, the more prone it becomes to several issues affecting the backpropagation algorithm (e.g. the vanishing gradient problem, \cite{vanishing1,vanishing2} the exploding gradient problem \cite{exploding} and the dead dead unit problem \cite{leaky,whyreludie,whenneuronsfail}) \cite{backprop}. 
\\\\
% Problema general
We claim that there are \emph{essentially} two families of methods to address these issues: \emph{architectural modifications} to the network and input data \emph{manipulation}. Examples of architectural modifications include (1) layer-width increase as done in \cite{wideresnet,inceptionv1}; (2) feed-forward structure network modification, as done in ResNets \cite{resnet} or DenseNets \cite{densenet}; (3) unit based alterations (of the their non-linearity or activation) as done in \emph{leaky}-\ReLU \cite{leaky} or \texttt{PReLU} \cite{prelu}). Meanwhile, the most commonly used data manipulation technique is \emph{data normalization} techniques on the output of layers as done under \emph{batch normalization} \cite{batchnorm}.  
\\\\
For example, layer-width increases reduces the chance for \emph{useless} units (e.g. dead or \emph{redundant}) per layer. However, such measures increase the computational cost of the network and the dimensionality of the problem, hampering convergence \cite{cursedim}. Meanwhile, modifying network connectivity may prove beneficial to tackle the vanishing gradient issue within a \cite{ladder,nin,highway}. This occurs however, at the expense of burdening designers with additional \emph{architectural} decisions (e.g. which connection to add or alter) taking a toll in parameter and hyper-parameter number as presented for example in \cite{densenet}. In turn, unit activation modification intends to ensure non-zero gradient back-propagation by (1) modifying unit functions to avoid zero truncation as done in \cite{leaky}, \cite{prelu}, \cite{elu} or \cite{selu} (\ReLU modifications) or modifying unit output \cite{crelu}. With batch normalization \cite{batchnorm}, several drawbacks have been described in works like \cite{batchrenorm} and \cite{batchnormGradientExplosion}: decreased performance in testing due to non-matching statistical parameters between training and testing (e.g. mean value and variance or non-i.i.d. minibatches or small batch sizes \cite{batchrenorm}; and exploding gradients limiting the depth of the network \cite{batchnormGradientExplosion}.
\\\\
% Problema específico (Pregunta del paper)
Since the existing methods require either computational expenditure and add arbitrary complexity to the architecture design, or simply entail limit network performance, we wonder whether we can train deeper networks without relying in any of those techniques. This imply using the minimum width possible, removing any additional connections or activations, and using no normalization. 
\\\\
% Solución propuesta (Respuesta a la pregunta)
We propose to achieve that by introducing a family of constraints which increase the separation performed by each of the units on the data. We demonstrate that doing so helps in avoiding trivial failure modes like dead or linear units. Additionaly, enables the use of zero initialization. The only added cost is an additional loss per layer and the inclusion of an additional hyperparameter. 
\\\\)
% Estructura del paper
This paper is organized as follows: section \ref{sec:separability} motivates the importance of enforcing separation in the units), after that section presents the \ref{sec:constraint} the actual constraints that we impose on the network to do so. Section \ref{sec:experiments} provides with experimental interpretation of the effect of the constraints and finally section \ref{sec:conclusions} closes the paper with the main conclusions.