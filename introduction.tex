\section{Introduction}\label{sec:introduction}
The success of Deep Learning has been linked to its ability to learn \emph{abstract representations} from input data in a hierarchical fashion  \cite{LeCun06atutorial,ramachandranEtAl2017SearchingForActivationFunctions}, using Deep Neural Networks (DNN for short). However, depth is also instrumental in various DNN \emph{issues} like the \emph{vanishing gradient} as presented in \cite{vanishing1} or \cite{vanishing2}; the \emph{exploding gradient} as presented in \cite{exploding}; the \emph{dead unit problem} as presented in \cite{leaky},\cite{whyreludie} or \cite{whenneuronsfail}; or the \emph{degradation problem} as presented in \cite{resnet}. 
\\\\
% Problema general
We claim we can categorize existing methods to address these issues in two families: \emph{architectural modifications} to the DNN and \emph{data manipulation}. Examples of architectural modifications include (1) layer-width increase as done in \cite{wideresnet,inceptionv1}; (2) additional connections (as done in ResNets \cite{resnet} or DenseNets \cite{densenet}); (3) unit augmentation (as done in \emph{leaky}-\ReLU \cite{leaky} or \texttt{PReLU} \cite{prelu}). Meanwhile, the most commonly used data manipulation technique is \emph{data normalization} on the output of layers as done under \emph{batch normalization} \cite{batchnorm}.  
\\\\
%occur amidst several geometric undertow \todo{we claim that these issues ocurr at geometric level, have a geometric background/interpretation}, that when unamanged, corrupt \todo{hinder} back-propagation. 

In the case of \ReLU based DNN, a geometric outlook allows us to conjecture on these issues differently. Under the setting defined by Rey-Torres et al. in \cite{reyRiera2019ModellingClassificationReLU},  gradient issues (vanishing and exploding) depend on \emph{dying-items} (e.g. dead units or points). In turn, dying items can be defined by  \emph{incorrect} positioning of the hyper-planes from the pre-activation of \ReLU units (\emph{relative} to the dataset). The reader can review our presentation of our rationale in Section \ref{sec:separability} on \emph{Separability}. 
\\\\
Thus, we can correct plane positioning by enforcing \emph{separation constraints} (\SepConstraint for short) as detailed in Section \ref{sec:constraint}) titled \emph{Separation Constraints}. Indeed, by targeting specifically the dead unit problem (defining the unit-based separation constraint \SepUnit); and the dead point problem (using a point-based separation constraint \SepPoint). Intuitively, \SepUnit constraints aim to secure unit pre-activation hyperplanes that \emph{cut} throughout the dataset (as detailed in Section \ref{sec:separability}). Meanwhile,  \SepPoint aims to secure the existence of units with opposing pre-activation values at each dataset point. 
\\\\
We test our proposal in a series of experiments (Section \ref{sec:experiments}) to showcase the effect of applying \emph{Separation constraints}. We find an increase on network depth using the same width than Batch Normalization \cite{batchnorm}, effectively reducing the dependency between depth and width as introduced in \cite{simpnet} and \cite{densenet}.  

\begin{table*}[h!]
\centering
% \begin{tabular}{cccc}
\begin{tabularx}{\textwidth}{lXlXlXlX}
\toprule
Family & Method & Examples & Intended issues\\ 
\midrule
Architectural & Width increase & \texttt{Wide-ResNet} \cite{wideresnet}, \texttt{Inception} \cite{inceptionv1} & Vanishing Gradient \newline Increase Depth\\
& Additional connections & \texttt{ResNet} \cite{resnet}, \texttt{DenseNet}\cite{densenet} & Vanishing Gradient \newline Increase Depth \\
& Unit augmentation & \emph{leaky}-\ReLU \cite{leaky}, \texttt{PReLU} \cite{prelu}, \texttt{C-ReLU} \cite{crelu} & Vanishing Gradient \newline Dead Units\\
Data manipulation & Modify layer output & \emph{Batch Normalization} \cite{batchnorm}  & Vanishing/Exploding Gradient\\                  
\bottomrule
\end{tabularx}
% \end{tabular}
\caption{Summary of techniques commonly used in deep learning to overcome issues like enhanced depth, vanishing/exploding gradient and dead neurons.}
\label{tab:techniquesTable}
\end{table*}

