\begin{abstract}
In Deep Learning, thin neural networks networks are especially prone to issues like dying neurons, overfitting, vanishing and exploding gradients. When using \ReLU architectures a series of linear separability  constraints  can  be  introduced  in order to (1)  allow  us  to  initialize  the  weights  to  zero, (2) control the aforementioned issues to a degree comparable to Batch Normalization; and (3)  enhance our geometric insight and construct interpretations of the dynamic behavior of DNN using elementary linear algebra.
\end{abstract}

\section{Introduction}\label{sec:introduction}

The success of Deep Learning has traditionally been linked to its ability to learn \emph{abstract representations} of data via function composition\cite{LeCun06atutorial}. Those functions (the \emph{layers}) decompose and solve the problem in \emph{parts}\cite{resnetSubtree}. The output of each layer is fed into the next until the output. However, if the network depth starts growing several problems appear which hamper the backpropagation process \cite{backprop}, like vanishing gradient \cite{vanishing1,vanishing2}; exploding gradient \cite{exploding} or dead units \cite{leaky,whyreludie,whenneuronsfail}. 

Traditionally, we identify four ways to address those problems. One solution is to increase the number of units per layer (\emph{width}), as we can see in \cite{wideresnet,inceptionv1}. Doing so increases the chance of having useful (not dead or redundant) units per layer by simple bruteforcing. However, this increases the computational cost of the network, as well as it increases the dimensionality of the problem, hampering convergence \cite{cursedim}.  Another is to modify the connectivity of the network so we ease the transfer of the gradient from the deeper layers to the input. In this school we find ResNets \cite{resnet}, DenseNets \cite{densenet} and others \cite{ladder,nin,highway}. The problem with this approach is that it introduces a large number of decisions to be made (which connection to use, from which layer to which layer, etc...), to the already large amount of hyperparameters of deep neural networks or increase the number of parameters as in \cite{densenet}. Finally, another way to do so is to alter the non-linearity used in the activation function so propagates back more gradient. In this trend we find examples like the leaky ReLU family in which the negevative part of the funcion has a non-zero gradient \cite{leaky,prelu}. Other variations include those employing functions of the exponential family for the negative part \cite{selu,elu} or more complex variations \cite{swish, srelu}. The main trait commons among all of them is that they do not truncate to zero, thus increasing the amount of gradient passed to the input layers. Another way to achieve the same objective is to augment the activation function by concatenating both positive and negative sides, like in \cite{crelu}, with the drawback of increasing the size of the units twofold in the following networks. Another alternative is to use normalization layers, with batch normalization as the most common \cite{batchnorm}. However, several drawbacks have been described about batchnorm like being sensitive to different staticstics between train and test, Non-i.i.d. minibatches and  and small batch sizes \cite{batchrenorm}, and causing exploding gradients which ultimately limit the depth of the network \cite{batchnormGradientExplosion}.

In this work we attempt to fix the network so we can train deeper networks without relying in any of those techniques, so we limit the available width as much as we can, refrain ourselves to use additional connections or activations, and remove any normalization scheme. We achieve that by introducing a family of constraints which ensure that the network performance is closer to the optimal by avoiding trivial failure models like dead or affine units. The only cost is the computing of an additional loss per layer and the inclusion of an additional hyperparameter. 