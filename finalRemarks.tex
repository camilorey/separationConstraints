\section{Final Remarks}\label{sec:finalRemarks}

In terms of performance, the network used was designed to fail with classical \ReLU and comparable improvement between batchnorm and the constraint trick yield similar improvements. Despite surprising accuracy levels up to 90\% on the \texttt{MOONS} dataset (against a 60\% from batchnorm), accuracy over \texttt{CIFAR}-10 yield only a meager 2\% increase against batchnorm (when used separately) and less than $2.2$\% when combined with batchnorm. Thus, accuracy-wise, the constraint formulation (specifically \texttt{SC-ReLU-L} is at most an \emph{alternative} to batch-norm in within the scope of our experimentation. However, the constraint formulation follows a sound logic of inception and yields a technique that allows interpretations involving \emph{elementary} mathematical elements, to more easily venture into explainability theory. In addition, the overwhelming success of \texttt{SC-ReLU-L} in the \texttt{MOONS} data suggests that the constraint formulation could be a \emph{nice} addition to low dimensionality (geometrically based) problems. 
\\\\
From an analytic point of view, the choice of $\pm(1-\xi)$ on the constraints (recall equations \ref{eq:constraintsForSCReLUU},\ref{eq:constraintsForSCReLUL} and \ref{eq:constraintsForSCReLUP}) ensure that layer function do not have non empty zero sets (however, not necessarily avoiding the dying neurons issue), and more interstingly, that they will have a non-negligible measure \cite{florenzano2001ConvexAnalysis}. While intuition leads us to believe that the \emph{larger} the zero sets are, the more \emph{probable} a separation will take place, a correct \emph{separation} falls to the responsibility of the total loss functional. Though the constraint formulation is loss \emph{agnostic}, further research needs to be done in matters of how \emph{sensitive} constraint inclusion using their $L_1$ was a fortuitous choice, and needs further observation.
\\\\
We theorize that this might be due the constraints transforming the internal arrangement of the network, by turning the hyperplanes lying inside the units so they comply better with the separability constraints.        
\\\\
In a similar note, the different choices of constraint types is in need of further investigation. Provided with topological facts like the non-emptynesss of zero sets (remark  \ref{remark:topologicalFacts}). Experimental results show that \ReLUL entails the largest accuracies (within the limited experimentation with \moons). While, the analytical conclusion of remark \ref{remark:relationBetweenConstraints} is that \ReLUU should yield best \emph{separability}, this begs the question over the relation between \emph{separability} and correctness within the constrain formulation and training loss.  
\\\\
Annecdotal evidence in our experimentation suggests that the constraint trick performs better with \emph{thin} networks (where the ratio between width and depth is significantly low). Experimentally this phenomenon was identified when making preliminary tests using \texttt{VGG-16} and the \texttt{WIDE-ResNet22}. From the point of view of our formulation the wider the layers become, the more hyperplanes are involved (via intersection) that shape the zero sets of layers, reducing their measure \cite{florenzano2001ConvexAnalysis}. On the one hand this could lead to possibly \emph{empty} zero sets at a rate that the constraints and the constraint loss cannot fathom, while on the other a \emph{smaller} zero set may provide a richer dynamic in terms of \emph{zero-set-to-support} motion, that may actually increase accuracy. 
\\\\
In addition to the non-obvious inability of the constraint formulation to serve as an actual Urysohn function, (a fact that cannot be elucidated from the formulation alone), the introduction of the constraints may compromise stability in the training process as Figure presents. A silver lining however emerges from the introduction of constraints throughout the experimentation: the accuracy drop between training and validation is reduced (which translates to a healthy measure of \emph{underfitting}), and with larger values of $\gamma$, the exploding gradient  at least delayed. 
\\\\
More specifically the constraint trick becomes positive towards avoiding overfitting when introducing  \emph{zero initialization}, as the comparison to the Glorot-initialization scheme was explored, as annecdotal evidence suggests (recall Figure Z). From an analytic point of view, such observation is justified as argued in subsection ZZ, while geometrically, remark \ref{remark:topologicalFacts} suggests that zero initialization and enforcing constraints clash in the first iterations of the learning process, quickly \emph{shaking} the values of  weights away from zero, where they can be regulated by the training loss functional, and perhaps by the introduction of other techniques such as \texttt{ADAM}.  
\\\\
In a similar direction, developing mathematical relations concerning the role of hyperparameters, amidst the zero vs. Glorot initialization, in matters of stability, convergence or accuracy are beyond the analytic capacities of our formulation and of our experimentation. Though the accuracy of zero init is (in all combination of learning rates chosen) superior, it remains unclear how $\lambda$ or batch size affect the zero initialization scheme, beyond annecdotal evidence. Moreover, the introduction of the constraint loss as presented in equation \ref{eq:constraintloss} as a sum can be modified to an \emph{average}, to balance the effect of the $\xi$.      
\\\\
The constraint formulation introduces a large quantity of additional parameters (twice in light of the double constraints mentioned in subsection \ref{subsec:constraintLoss}) when implementing the minimization problem \ref{eq:constraintLossFunctional}. Notice for example that for \texttt{SC-ReLU-U} we need to burden processors with slack variables equal -in size- to the number of neurons of the training set, while \texttt{SC-ReLU-P} depends on batch size. In light of only \emph{marginal} improvements in matters of accuracy, the constraint trick may be  \emph{disappointing}. However, we remind the reader of remark \ref{remark:relationBetweenConstraints} that signals \texttt{SC-ReLU-L} as our biggest hope: (1) it guarantees no dead neurons by virtue of remark \ref{remark:topologicalFacts} and (2) it demands us to introduce only a number of constraints equal to the depth of the network. 
  