\section{Conclusions}\label{sec:conclusions}

The constraints introduced in \ref{sec:constraint} effectively enable a feedforward \ReLU-based network to perform adequately disregarding the depth of the network, without resorting to any of the common solutions to enable higher depth in deep neural networks, namely increasing \emph{width}, adding additional connections, modifying the activation function, or using normalization layers. This allows to simplify the number of decisions needed to be taken when designing the network.

We explore a family of constraints starting with our initial \SepUnit constraint, which we find not sufficient in \ref{subsec:sepUnit} and we augment with \SepPoint in \ref{subsec:sepUnitPoint}. We also propose a lower cost computationally expensive version \SepLayer in \ref{subsec:sepLayer}, which performs surprisingly well in \ref{subsec:classification}.

Empirical proof is provided in \ref{sec:experiments} using a network of only 4 units per layer, showing how our proposal kind of linearizes the internal representations so gradient can be forwarded backwards better, see section \ref{subsec:classification}. We find similarity of our experimental results to \cite{hauserAsok}, since apparently our
constraints make the transitions between layers smoother. In the other hand, we also find to \cite{batchnormGradientExplosion}, as we see how the network performs better the closest it is to the linear regime. Additionally, we see in \ref{subsec:effectConstraintLoss} how our constraints force the network to preserve better topological structure such as connectivity, so during training the gradient of the loss functional is better propagated backwards. We also present hints of the failure modes of \ReLU and \ReLUBN, by showing how the first sends all the points to zero as the network grows deep, and the second mixes them in two points, resulting in \emph{topological mixing}. Nevertheless, instability shown in \ref{fig:peaks} requires further research, along with some failure modes where \SepUnitPoint is unable to converge, or some other where \SepLayer converges to zero yet the network is unable to learn.

Finally, we explore the application of our constraints to the problem of zero initialization, using Annealed Dropout to break symmetry of the weights, and showing promising results, although more work in regards to the stochasticity introduced by Dropout is required.