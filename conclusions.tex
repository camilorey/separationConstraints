\section{Conclusions}\label{sec:conclusions}

% PREGUNTA DEL PAPER
% Since the existing methods require either computational expenditure and add arbitrary complexity to the architecture design, or simply entail limit network performance, we wonder whether we can train deeper networks without relying in any of those techniques. This imply using the minimum width possible, removing any additional connections or activations, and using no normalization. 

% 1.1
The separation constraint is \emph{comparable} to the usual techniques (as presented in the introduction). Indeed, \SepLayer allows for affine units (recall Equation \ref{eq:affineUnit}) that forward representations between layers in the same spirit as ResNets \cite{resnet} or DenseNets \cite{densenet}. Meanwhile, a layer satisfying the \SepPoint constraint ensures the existence of two units with opposite-facing hyperplanes similarly to \texttt{C-ReLU} \cite{crelu}. In addition, the intuition of the separation constraint method intends to serve as stronger heuristic for parameter configuration (independently of initial values), so that units reach a useful configuration for backpropagation, instead of insisting on reaching on some \emph{favorable} initial configuration via trial-and-error as done in the layer-width-increase approach \cite{wideresnet,inceptionv1}.   
\\\\
% 2.1
We claim that there is a difference between non-zero and \emph{useful} activations. As presented in Figure \ref{fig:moonsReLUBN}, \ReLUBN induces a solution that maps the dataset non-trivially throughout the network. However, its performance on the \texttt{MOONS} dataset is inferior to our proposal (63\% of accuracy as presented in Table \ref{tab:moons} against any constraint based test over 90\%). Moreover, comparing the output graph of \ReLUBN (Figure \ref{fig:moonsReLUBNOutput}) with our findings (Figures \ref{fig:moonsUnitwiseOutput}, \ref{fig:moonsPointwiseOutput}. \ref{fig:moonsLayerwiseOutput} and \ref{fig:moonsUnitpointwiseOutput}) the reader can observe that the separation reached separates components of the \texttt{MOONS} dataset in intuitive manners.  
\\\\
% 3.1
The separation constraints compels the data to go through the network favourably for the cross-entropy loss back-propagation, see Section \ref{subsec:sepvsxent}. Analytically, we can elaborate on this behavior if we are reminded of the fact that the gradient of the main loss is non-zero \emph{on} the upper sets of units by virtue of Equation \ref{eq:upperPartOfUnit} and the separability conditions presented in subsection \ref{subsec:ReLUSeparability}. Thus, ensuring non-empty upper sets of units is a sufficient condition for back-propagation. 
\\\\
However, further inquiry into the interaction between constraint loss and main loss beyond parameter $\lambda$ is needed, alongside with experiments regarding other loss functionals besides the cross-entropy loss. In addition,  the use of the separation constraints in other types of layers such as convolutional \cite{lenet}, LSTM \cite{lstm} or transformers \cite{transformer}\cite{transformer2} is yet to be performed, in order to explore other types of tasks like regression, unsupervised learning \cite{embedding}, graph \cite{graph} or generation \cite{gan,vae}. Morever, more challenging datasets are definitely in need of revision.
\\\\
% 4.1
While the separation constraints prevent the vanishing gradient, the \emph{exploding gradient} remains at large. Notice that constraint introduction places \emph{lower bounds} on the magnitude of the pre-activation values, but does not place upper bounds. This could be solved introducing additional constraints or limiting the norms of weight vectors. 
\\\\
Analytically, we can approach affine units in the same spirit as \texttt{ResNet} \cite{resnet} or \texttt{DenseNet}\cite{densenet}. Since the composition of affine functions is affine, collections of successive (across the layers) affine units effectively \emph{shortcut} the network. Experimentally, we verify this in our \SepLayer experiments (as it is the only separation constraint that allows affine units). Notice that there is a coalescence between axis-aligned and diagonal units that preserves the topology of the intermediate representation of the dataset after layer 4, see Figures \ref{fig:moonsLayerwise42} and \ref{fig:moonsLayerwiseFeature2} are almost equal. We conjecture that repeated units perform the same role for \SepUnit, since affine units are forbidden with this constraint. Notice how in Figure \ref{fig:moonsUnitwise252} all the planes are cutting at the same point, thus having the same \emph{upper} and \emph{lower} sets. Interestingly, those sets are compressed into two points in the feature layer (Figures \ref{fig:moonsUnitwiseFeature1} and \ref{fig:moonsUnitwiseFeature2}), but if we add \SepPoint as in \SepUnitPoint we find the same behaviour again, where the feature layer is similar to the inner representation at layer 25 (Figures \ref{fig:moonsUnitpointwise252} and \ref{fig:moonsUnitpointwiseFeature2}). 
\\\\
We explore the use of separation constraint (\SepUnitPoint in this case) to enable zero initialization with good results, (recall subsection \ref{subsec:zero}). In order to make it work, we had to introduce to break symmetry in the weights, in the form of Annealed Dropout \cite{dropoutAnnealing}. Since our objective in this matter (along with the entire paper) was simplicity, the use of Dropout partially defeats this purpose by moving the same stochasticity that we were removing from intialization into training. Nevertheless, our \emph{Ansatz} is that an initialization that is based on the data (which is the result of using the constraint plus Dropout) must be superior to any random initialization which is only based in features like gradient magnitude or inputs and outputs. Further research in this matter is hereby required.
\\\\
% World peace
We hope that the geometric framework developed in this work will help us to delve deeper into the intricacies of deep learning so even further simplification is achieved. 




