\section{Final Remarks}\label{sec:finalRemarks}

In terms of performance, the use of the separation constraint is able to bring simple \ReLU activation to parity with respect Batch Normalization. In the case of the \texttt{MOONS} dataset we see that the network is capable to to solve perfectly the problem (against a 60\% accuracy from batchnorm). However, accuracy over \texttt{CIFAR}-10 yield only a meager 2\% increase against batchnorm (when used separately) and less than $2.2$\% when combined with batchnorm. Nevertheless, the constraint formulation yields a technique that allows interpretations involving \emph{elementary} mathematical elements, to more easily venture into explainability theory. In addition, the overwhelming success of \texttt{SC-ReLU-L} in the \texttt{MOONS} data suggests that the constraint formulation could be a \emph{nice} addition to low dimensionality (geometrically based) problems. 
\\\\
Anecdotal evidence in our experimentation suggests that, whereas the separation constraint performance is similar when using a production-grade network, it surpasses its competitors in \emph{thin} networks where they simply fail (where the ratio between width and depth is significantly low). Experimentally this phenomenon was identified when making preliminary tests using \texttt{VGG16} and the \texttt{WideResNet22}. From the point of view of our formulation the wider the layers become, the more hyperplanes are involved (via intersection) and less critical is having dead/redundant units.
\\\\
More specifically the separation constraint becomes positive towards avoiding overfitting when introducing  \emph{zero initialization}, as the comparison to the Glorot-initialization scheme was explored, as annecdotal evidence suggests (recall Figure Z). From an analytic point of view, such observation is justified as argued in subsection \ref{subsec:accuracyResults}, while geometrically, remark \ref{remark:topologicalFacts} suggests that zero initialization and enforcing constraints clash in the first iterations of the learning process, quickly \emph{shaking} the values of  weights away from zero, where they can be optimized by the training loss.
\\\\
In a similar direction, developing mathematical relations concerning the role of hyperparameters, amidst the zero vs. Glorot initialization, in matters of stability, convergence or accuracy are beyond the analytic capacities of our formulation and of our experimentation. Though the accuracy of zero initialization is (in all combination of learning rates chosen) superior, it remains unclear how $\lambda$ or batch size is involved, beyond anecdotal evidence. Moreover, the introduction of the constraint loss as presented in equation \ref{eq:constraintloss} as a sum can be modified to an \emph{average}, to balance the effect of the $\xi$.      
\\\\
The problem of what to do with redundant units is left open, as apparently they are needed in order to forward the output of intermediate layers to the top of the network. Solutions like using skip connections or concatenation like in \ref{resnet} or \ref{densenet} can alleviate the problem, but we think that the optimal would be to remove them directly. Thus, a pruning scheme for them is left for future research.
  