\section{Conclusions}\label{sec:conclusions}

% PREGUNTA DEL PAPER
% Since the existing methods require either computational expenditure and add arbitrary complexity to the architecture design, or simply entail limit network performance, we wonder whether we can train deeper networks without relying in any of those techniques. This imply using the minimum width possible, removing any additional connections or activations, and using no normalization. 

% 1.1
The separation constraint is \emph{comparable} to the usual techniques (as presented in the introduction). Indeed, \SepLayer allows for affine units (recall Equation \ref{eq:affineUnit}) that forward representations between layers in the same spirit as ResNets \cite{resnet} or DenseNets \cite{densenet}. Meanwhile, a layer satisfying the \SepPoint constraint ensures the existence of two units with opposite-facing hyperplanes similarly to \texttt{C-ReLU} \cite{crelu}. In addition, the intuition of the separation constraint method intends to serve as stronger heuristic for parameter configuration (independently of initial values), so that units reach a useful configuration for backpropagation, instead of insisting on reaching on some \emph{favorable} initial configuration via trial-and-error as done in the layer-width-increase approach \cite{wideresnet,inceptionv1}.   
\\\\
% 2.1
We claim that there is a difference between non-zero and \emph{useful} activations. As presented in Figure \ref{fig:moonsReLUBN}, \ReLUBN induces a solution that maps the dataset non-trivially throughout the network. However, its performance on the \texttt{MOONS} dataset is inferior to our proposal (63\% of accuracy as presented in Table \ref{tab:moons} against any constraint based test over 90\%). Moreover, comparing the output graph of \ReLUBN (Figure \ref{fig:moonsReLUBN} parts (f) and (g)) with our findings (Figures \ref{fig:moonsUnitwise}, \ref{fig:moonsLayerwise} and \ref{fig:moonsUnitpointwise}) the reader can observe that the separation reached separates components of the \texttt{MOONS} dataset in intuitive manners.  
\\\\
% 3.1
The separation constraints compels the data to go through the network favorably for the cross-entropy loss back-propagation. Experimentally, Figure \ref{fig:peaks} suggests that the training phase begins with minimizing the constraint loss until a certain level (approximately $0.21$ around epoch 750), enabling the optimization of the main loss (recall 80 to 100\% accuracy in our tests in Table \ref{tab:moons}). Analytically, we can elaborate on this behavior if we are reminded of the fact that the gradient of the main loss is non-zero \emph{on} the upper sets of units by virtue of Equation \ref{eq:upperPartOfUnit} and the separability conditions presented in subsection \ref{subsec:ReLUSeparability}. Thus, ensuring non-empty upper sets of units is a sufficient condition for back-propagation. 
\\\\
However, further inquiry into the interaction between constraint loss and main loss beyond parameter $\lambda$ is needed. Anecdotal evidence shows spurious transient states on convergence in the cases of \SepPoint, \SepUnit and \SepUnitPoint, but not on \SepLayer.  In the scope of our experimentation we have chosen the cross-entropy loss functional, further inquiry is required on the choice of main loss. 
\\\\
% 4.1
While the separation constraints prevent the vanishing gradient, the \emph{exploding gradient} remains at large. Notice that the constraints designed place \emph{lower bounds} on the magnitude of the pre-activation values, it places no upper bound. Analytically, this gradient depends on both on the magnitude of the weight vectors, absolute value of the biases and  the magnitude of dataset points mapped throughout the network. This could be solved introducing additional constraints or limiting the norms of weight vectors. 
\\\\
Despite the fact that we intended to avoid dead and affine neurons with our geometrical formulation (recall the predicate $R_1(u) \wedge R_2(u)$ defined on Equation XXX) the different constraints  object in practice. While $R_1(u) \wedge R_2(u)$ is valid within \SepUnit and from it, it extends to layers, \SepLayer and \SepPoint,  allow at least on unit to be affine  over the dataset (recall Equation \ref{eq:affineUnit}) and at least another to be dead (recall Equation \ref{eq:deadNeuronVersion2}), invalidating $R_1(u) \wedge R_2(u)$ for each unit in a layer $\layer$, but holding $R_1(\layer) \wedge R_2(\layer)$. 
\\\\
In this sense, neither \emph{repeated} (producing the same upper set), affine nor dead units are \emph{harmful} for network performance, beyond burdening networks with additional non used parameters. They seem ubiquitous in the extent of our experimentation as Figures \ref{fig:moonsLayerwise}, \ref{fig:moonsPointwise} and \ref{fig:moonsUnitpointwise} testify (see the axis-aligned or diagonally expanding intermediate representations of the dataset). Further inquiry is needed to understand how dead, affine and repeated neurons coalesce in solving the problem. In addition, our experimentation suggests that the  the distribution of dead, affine and repeated units varies according to constraint type (in the extent of our experimentation). 
\\\\
Indeed, Figure \ref{fig:moonsLayerwise} for the \SepLayer constraint type showcases a feature layer (divisions \ref{fig:moonsLayerwiseFeature1} and \ref{fig:moonsLayerwiseFeature1}) with two dead units and two repeated. Meanwhile Figure \ref{fig:moonsUnitwise} at the 24th layer (divisions \ref{fig:moonsUnitwise251} and \ref{fig:moonsUnitwise252}), depicts repeated neurons (organized pairwise) with no affine nor dead units. 
\\\\
Analytically, we can approach affine units in the same spirit as \texttt{ResNet} \cite{resnet} or \texttt{DenseNet}\cite{densenet}. Since the composition of affine functions is affine, collections of successive (across the layers) affine units effectively \emph{shortcut} the network. Experimentally, we verify this in our \SepLayer experiments (as it is the only separation constraint that allows affine units). Notice that there is a coalescence between axis-aligned and diagonal units that preserves the topology of the intermediate representation of the dataset after layer 4, see Figures \ref{fig:moonsLayerwise42} and \ref{fig:moonsLayerwiseFeature2} are almost equal. We conjecture that repeated units perform the same role for \SepUnit, since affine units are forbidden with this constraint. Notice how in Figure \ref{fig:moonsUnitwise252} all the planes are cutting at the same point, thus having the same \emph{upper} and \emph{lower} sets. Interestingly, those sets are compressed into two points in the feature layer (Figures \ref{fig:moonsUnitwiseFeature1} and \ref{fig:moonsUnitwiseFeature2}), but if we add \SepPoint as in \SepUnitPoint we find the same behaviour again, where the feature layer is similar to the inner representation at layer 25 (Figures \ref{fig:moonsUnitpointwise252} and \ref{fig:moonsUnitpointwiseFeature2}). 
\\\\
We use the separation constraint (\SepUnitPoint in this case) to explore the use of zero initialization with good results, see Section \ref{subsec:zero}. In order to make it work though, we had to introduce  means to break the symmetry between the positive and negative constraint ,in the form of $\rho$ (see Equation \ref{eq:definitionOfRho}), and to break symmetry in the weights, in the form of Annealed Dropout \cite{dropoutAnnealing}. Since our objective in this matter (along with all the paper) was simplicity, and the use of Dropout partially defeats this purpose by moving the same stochasticity that we were removing from intialization into training. Nevertheless, our \emph{ansatz} is that an initialization that is based on the data (which is the result of using the constriant plus Dropout) must be superior to any random initialization which is only based in features like gradient magnitude or inputs and outputs. Further research in this matter is hereby required.






