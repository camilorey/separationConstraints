% Hypothesis
% \begin{itemize}
%     \item Separability is important
%     \begin{itemize}
%         \item Separation sets
%         \item All positive network
%     \end{itemize}
%     \item Controlling separability enhances the network
%     \begin{itemize}
%         \item Depth
%         \item Dead units
%         \item Zero init
%         \item Moons data shattering
%     \end{itemize}
%     \item Separability works in real world
%     \begin{itemize}
%         \item Toy Cifar
%         \item SimpNet
%         \item ULMfit
%         \item Fixup
%     \end{itemize}
% \end{itemize}

\section{Experiments and Results}\label{sec:experiments}
After presenting the separability constraints, we test our formulation in an example of a \ReLU-based network (of depth $d=50$, and fixed layer width $N_k=4$), specially targeted to: 
\begin{enumerate}
    \item fail to trivial accuracy values with \ReLU and batchnorm.
    \item exhibit the highlighted issues that DNN experience: dying neurons, vanishing gradient, and to some extent exploding gradient.
    \item is manageable enough to relate to analytical results presented previously.
\end{enumerate}
Experimentation was done over two datasets: the \moons dataset (for \emph{toying}) and the \texttt{CIFAR-10} to test in a more realistic setting involving (a CNN with $5\times 5$ kernels with padding to preserve image size), using \texttt{Keras} and \texttt{TensorFlow}.
\\\\
Within the \moons dataset we sampled $100$ points ($85$ for training and $15$). As hyper-parameters, we used learning rates $\gamma \in \{0.01, 0.001, 0.0001\}$, a batch size $bs = 85$, to ensure \emph{fairness} for \SepUnit.  For testing, the loss functional $\mathcal{L}$ in equation \ref{eq:constraintLossFunctional}  was chosen to be the cross-entropy as presented in \cite{LeCun06atutorial}, introducing constraint loss parameter $\lambda = 0.01$ using $T = 2000$ epochs. 
\\\\
Due to the fact that \cifar is a much larger dataset, choosing hyper-parameters for experimentation were chosen empirically. Learning rates are \emph{smaller} than the ones used for \moons: $\gamma \in \{0.001, 0.0005, 0.0001 \}$, while extending experimentation to two batch sizes: $64$ and $128$.  Constraint trade-off parameter $\lambda$ was tuned using: $10^{-5},10^{-3}$ and $1$ values, and for the sake of speed we set the number of epochs to $T = 500$.
\\\\
In matters of \emph{initialization} two schemes were tested: the \emph{Glorot Uniform Scheme} implemented in \texttt{Keras} following (see \cite{Glorot10Initialization} for the technical details) and \emph{zero initialization} consisting in setting weights and biases to zero. 

\subsection{Visualization}

In order to understand the effect that separability has we use the \moons dataset in order to visualize its dynamics. We train a network composed of 50 layers

\begin{figure*} 
    \centering
  \subfloat[\ReLU input \label{fig:reluInput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/relu/0.pdf}}
  \subfloat[\ReLU second layer \label{fig:relu2}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/relu/2.pdf}}
  \subfloat[\ReLU 21th layer \label{fig:relu21}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/relu/21.pdf}}
  \subfloat[\ReLU 46th layer \label{fig:relu46}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/relu/46.pdf}}
  \subfloat[\ReLU output\label{fig:reluOutput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/relu/output.pdf}}
\\
  \subfloat[\ReLUBN input \label{fig:reluBNInput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/relu-bn/0.pdf}}
  \subfloat[\ReLUBN 4th layer\label{fig:reluBN4}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/relu-bn/4.pdf}}
   \subfloat[\ReLUBN 21th layer\label{fig:reluBN21}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/relu-bn/21.pdf}}
  \subfloat[\ReLUBN 48th\label{fig:reluBN48}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/relu-bn/48.pdf}}
  \subfloat[\ReLUBN output\label{fig:reluBNOutput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/relu-bn/output.pdf}}
\\
  \subfloat[\SepLayer input\label{fig:layerwiseInput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/layerwise/0.pdf}}
  \subfloat[\SepLayer 4th layer \label{fig:layerwise4}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/layerwise/4.pdf}}
  \subfloat[\SepLayer25 layer\label{fig:layerwise25}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/layerwise/25.pdf}}
   \subfloat[\SepLayer 49th layer \label{fig:layerwise49}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/layerwise/49.pdf}}
  \subfloat[\SepLayer\label{fig:layerwiseOutput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/layerwise/output.pdf}}
\\
  \subfloat[\SepUnit input \label{fig:unitwiseInput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitwise/0.pdf}}
  \subfloat[\SepUnit 4th layer\label{fig:unitwise4}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/unitwise/4.pdf}}
  \subfloat[\SepUnit 23th layer\label{fig:unitwise23}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitwise/23.pdf}}
   \subfloat[\SepUnit 49th layer\label{fig:unitwise49}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitwise/49.pdf}}
  \subfloat[\SepUnit output\label{fig:unitwiseOutput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitwise/output.pdf}}
\\
  \subfloat[\SepPoint input \label{fig:pointwiseInput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/pointwise/0.pdf}}
  \subfloat[\SepPoint 4th layer\label{fig:pointwise4}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/pointwise/4.pdf}}
  \subfloat[\SepPoint 24th layer\label{fig:pointwise24}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/pointwise/24.pdf}}
   \subfloat[\SepPoint 49th layer\label{fig:pointwise49}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/pointwise/49.pdf}}
  \subfloat[\SepPoint output\label{fig:pointwiseOutput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/pointwise/output.pdf}}
\\
  \subfloat[\SepPointUnit input \label{fig:upwiseInput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitpointwise/0.pdf}}
  \subfloat[\SepPointUnit 4th layer\label{fig:upwise4}]{%
        \includegraphics[width=0.20\linewidth]{img/toy/unitpointwise/4.pdf}}
  \subfloat[\SepPointUnit 25th layer\label{fig:upwise25}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitpointwise/25.pdf}}
   \subfloat[\SepPointUnit 48th layer\label{fig:upwise48}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitpointwise/48.pdf}}
  \subfloat[\SepPointUnit output\label{fig:upwiseOutput}]{%
       \includegraphics[width=0.20\linewidth]{img/toy/unitpointwise/output.pdf}}
       
  \caption{Visualization of a 50x4 network trained on the moons dataset with batch size equal to the entire training set. The first column shows the data space with the partitions performed by the first layer (a), (f), (k), (p), (u)). The blue lines show the separating plane whereas the gray zone accounts for the zero area of each of the 4 units. The three following columns show the projection of the dataset in the space of the two first units of a selected layer: bottom ((b), (g), (l), (q), (v), (aa)), middle ((c), (h), (m), (r), (w), (ab)), and top layers ((d), (i), (n), (s), (x), (ac)). Finally, the last column shows the output surface of the network in the original data space ((r), (j), (o), (t), (y), (ad)). Notice how the gradient has changed the separation performed by \ReLU in the input layer (a), but it fails to find a proper representation of use for the upper layers ((b), (c), (d)), so starting from the middle of the network it sends the entire dataset to zero. This leads to the total failure to output anything useful (j) which will make difficult to get any kind of \emph{meaningful} gradient in order to escape from this situation. We argue that this is due the fact that the lower layers (b) are breaking the \emph{topological structure} of the classes of the dataset, making the gradient of the respective layers so \emph{unrelated} with the original structure that fails in fixing the unsuitable partition generated by the random initialization. \ReLUBN fares a bit better since it is able to pull the data out of zero, as we can see in the upper layers ((h), (i)), yet it is still unable to preserve the \emph{topological structure} like \ReLU, resulting in \emph{data shattering} and \emph{mezcla topologica}. This generates very strange outputs which we can see in (j) whose gradients fail again to provide hints to the lower layers to fix the topological structure of the network. In the other hand, our relaxed version of our proposal \SepLayer is able to solve the problem (o). We find how is able to propagate the information needed for the lower layers to perform the separation, so we can see at (k) how the hyperplanes are much better placed, so by the bottom layers (l) the problem is already solved and the rest of layers simply forward upwards ((m), (n)). Note how as our formulation is relaxed from \SepUnit there are dead units (m), note the vertical axis showing how the horizontal unit is effectively dead. Also notice how the entire space is colored in gray, showing how at least one unit is effectively operating in linear mode. This indicates the role of the upper layers on forwarding the already found solution. The fourth row shows our restricted version \SepUnit, which forces all the units of the network to separate. Note how the non-linear performance of \ReLU is directly linked to its separation, forcing all the units to separate equals to forcing all the units to be non-linear. This harms the forwarding role shown in \SepLayer, resulting in that although the input layer (p) shows a sensible configuration, as the data goes upwards (r), either the constraint is violated (even sightly) or something goes the wrong way, but the network fails to output anything \emph{meaningful} (t). Notice how event the network fails, the respresentation found by the network still propagates all way to the input layer, hinting that it could be used as a pretraining step. The next row shows the dual of our constraint \SepPoint. We see how it effectively propagates the data \emph{topological structure} up to the output of the network ((v), (w), (x)), but as we are only requiring the extrema of the pre-activations of each layer to be either -1 or 1, there is no requirement to be perform a non-linear function, so the solution remains almost linear (y). Finally, if we try to balance the excessive non-linearily of \SepUnit with excessive linearity of \SepPoint we can combine them in \SepPointUnit, which performs adequately but its solution (ad) is a bit more complex than \SepLayer (o). Notice how each of the units is effectively separating the data ((aa), (ab), (ac)). The solution found at the first layer (z) is also a bit different. Nevertheless, it is able to overcome the issues presented by \SepUnit by forcing to separate points which were already separated, thus becoming redundant and effectively forwarding the previous solution found at the input. In overall, we find how the constraints enable to propagate a gradient which apparently is better to adjust the lower layers than their counterparts.}
  \label{fig:toy} 
\end{figure*}


Init

\begin{figure*} 
    \centering
  \subfloat[\ReLU input \label{fig:reluInputInit}]{%
       \includegraphics[width=0.20\linewidth]{img/init/relu/0.pdf}}
  \subfloat[\ReLU feature layer \label{fig:reluInputFeatureInit}]{%
        \includegraphics[width=0.20\linewidth]{img/init/relu/49.pdf}}
  \subfloat[\ReLUBN input at epoch 40 \label{fig:relubnInputInit}]{%
        \includegraphics[width=0.20\linewidth]{img/init/relu-bn/0.pdf}}
  \subfloat[\ReLUBN feature layer at epoch 40 \label{fig:relubnFeatureInit}]{%
       \includegraphics[width=0.20\linewidth]{img/init/relu-bn/48.pdf}}
       \\
  \subfloat[\SepLayer input at epoch 0 \label{fig:sepInitInput0}]{%
       \includegraphics[width=0.20\linewidth]{img/init/layerwise/0/0.pdf}}
  \subfloat[\SepLayer feature layer at epoch 0 \label{fig:sepInitFeature0}]{%
        \includegraphics[width=0.20\linewidth]{img/init/layerwise/0/49.pdf}}
  \subfloat[\SepLayer input at epoch 16 \label{fig:sepInitInput16}]{%
       \includegraphics[width=0.20\linewidth]{img/init/layerwise/16/0.pdf}}
  \subfloat[\SepLayer feature layer at epoch 16 \label{fig:sepInitInput16}]{%
        \includegraphics[width=0.20\linewidth]{img/init/layerwise/16/49.pdf}}
        \\
  \subfloat[\SepLayer input at epoch 32\label{fig:sepInputInit32}]{%
       \includegraphics[width=0.20\linewidth]{img/init/layerwise/32/0.pdf}}
  \subfloat[\SepLayer feature layer at epoch 32\label{fig:sepInputInit32}]{%
        \includegraphics[width=0.20\linewidth]{img/init/layerwise/32/49.pdf}}
\subfloat[\SepLayer input at epoch 48\label{fig:sepInputInit48}]{%
       \includegraphics[width=0.20\linewidth]{img/init/layerwise/48/0.pdf}}
  \subfloat[\SepLayer feature layer at epoch 48 \label{fig:sepInputInit48}]{%
        \includegraphics[width=0.20\linewidth]{img/init/layerwise/48/49.pdf}}

  \caption{This figure shows the input and feature layers of network composed of 50 layers of 4 units each. The network is trained without loss, implying that for \ReLU it is left as it was initialized and in the case of \ReLUBN a number of epochs is run in order to ensure that the moments are adjusted. We train also \SepLayer for a number of epochs in order to understand what changes are undergoing in the network if we only optimize the separation constraint, without no additional loss. We find that the Glorot initialization used with \ReLU is quite poor with regards to the resulting representation in the feature layer, as we see in (b). As we see in \ref{fig:toy}, \ReLU tends to pack all the data to zero as the network grows in depth, disregarding the initialization. \ReLUBN is able to push some data from zero, see (d), but we argue that this is product of the biases of the network, other than a real connection with the data. This leads to the fact that the input layer remains the same than \ReLU's, see (c). If we look at \SepLayer (e-l), we see how at the beginning ((e), (f)) the starting point is indeed the same than \ReLU, but incrementally thanks to the separating constraint, it pushes the hyperplanes all the way back to the input (g) so it is able to have a much better representation at (j).} 
  \label{fig:init} 
\end{figure*}

\subsection{Results concerning Accuracy}\label{subsec:accuracyResults}
To gauge accuracy between \ReLU, \ReLU +  batchnorm (\texttt{\ReLU+BN}), and \SepUnit we tested all variants of \SepUnit as described on definition \ref{def:separationConstraints} for the \moons dataset, introducing also  \SepPointUnit combining \SepUnit and \SepPoint. \SepUnit shows a disparity in improvements against batchnorm, between both datasets, but shows significant improvements over the basic \ReLU baseline (that yields \emph{trivial} accuracies of $40$\% in \moons and $10$\% in \cifar), under the Glorot initialization scheme. The reader can observe Table \ref{tab:moons} for \moons (where all \SepConstraint variants were tested) and Table \ref{tab:cifar10} for \cifar (displaying only \SepLayer) for verification. 
\\\\
In the \moons dataset constraint introductions demonstrate \emph{perfect} accuracy, whereas batchnorm falls behind to $60$\%.  In contrast maximal \cifar accuracy yielded a $56.82$\% using \SepLayer, against a $56.03$\% maximal accuracy of batchnorm. In matters of the differences between the variants of the constraints, testing done with the \moons dataset (as presented in Table \ref{tab:scopes}, \SepLayer achieves maximal accuracy in the \moons dataset.  When combining batchnorm and constraints (of \SepLayer type) a maximal accuracy of $58.80$\% is reached using batch sizes of $64$ and $128$.  
\\\\
When considering \emph{overfitting}, \SepLayer shows a reduction in overfitting in contrast to \ReLUBN (20\% in \moons and 30\% for \cifar), while \SepLayer shows a drop of $3\%$ in \cifar and $0$\%. When changing the initialization scheme to zero in \cifar using \SepLayer we observe that accuracy increases to $58.12$\%. \ReLUBN and \ReLU \emph{cannot} function using zero initialization. In addition, Figure \ref{fig:ZeroVsGlorotDifference} displays the gaps between train and validation accuracy for the \cifar dataset as a histogram (over 44 runs, discarding experiments with accuracies lower than $30$\%), while Figure \ref{fig:zeroConvergence}, shows that zero initialization delays the exploding gradient breakdown in accuracy, a phenomenon that is observed using the highest learning rate for \cifar, $\gamma=0.001$. 
\begin{table}[h!]
\begin{center}
\begin{tabular}{l|rr|rr}
\toprule
{}  & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Loss} \\
{}  & Train   & Val.  & Train  & Val.  \\
\midrule
\ReLU            &  0.5176 &      0.4 &  0.6925 &  0.6938 \\
\ReLUBN     &  0.8117 &      0.6 &  0.6331 &  0.6636 \\
\SepUnit &  1.0000 &      1.0 &  0.0000 &  0.0211 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Maximal performance results for the toy experiment using the \moons dataset. From left to right, accuracy and loss (for \emph{train} and \emph{validation} sets) for \ReLU, \ReLUBN, and  \SepUnit in all its variants.}
  \label{tab:moons}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l|rr|rr}
\toprule
{}  & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Loss} \\
{}  & Train   & Val.  & Train  & Val.  \\
\midrule
\ReLU              &  0.0970 &   0.1000 &  2.3025 &  2.3025 \\
\ReLUBN            &  0.8300 &   0.5603 &  0.4614 &  1.2391 \\
\SepUnit*    &  0.6172 &   0.5812 &  1.0665 &  1.1867 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Maximal performance results for the \cifar dataset. From left to right, accuracy and loss (for \emph{train} and \emph{validation} sets) for \ReLU, \ReLUBN, and  \SepUnit in all its variants. (*) Zero initialization for \SepUnit was used. }
  \label{tab:cifar10}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{l|rr|rr}
\toprule
{}  & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Loss} \\
{}  & Train   & Val.  & Train  & Val.  \\
\midrule
\SepLayer    &  1.0000 &  1.0000 &  0.0000 &  0.0211 \\
\SepPoint    &  0.9294 &  0.8000 &  0.1765 &  0.6476 \\
\SepUnit    &  0.9058 &  0.8000 &  0.4161 &  1.5228 \\
\SepPointUnit   &  0.9882 &  0.9333 &  0.6988 &  1.0810 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Comparison between maximal performances of all \SepUnit variants using \moons dataset. From left to right, accuracy, and loss are presented (for \emph{train} and \emph{validation} sets).}
  \label{tab:scopes}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{l|rr|rr}
\toprule
{}  & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Loss} \\
{}  & Train   & Val.  & Train  & Val.  \\
\midrule
Glorot &  0.6739 &   0.5682 &  0.9114 &  1.2673 \\
Zeros  &  0.6172 &   0.5812 &  1.0665 &  1.1867 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Maximal performance comparison between initialization schemes using the \cifar dataset. From left to right, accuracy and loss (for \emph{train} and \emph{validation} sets) for Glorot and Zeros initializations.}
  \label{tab:zero_cifar}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{l|rr|rr}
\toprule
{}  & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Loss} \\
{}  & Train   & Val.  & Train  & Val.  \\
\midrule
\ReLUBN           &  0.8300 &   0.5603 &  0.4614 &  1.2391 \\
\SepUnit    &  0.6739 &   0.5682 &  0.9114 &  1.2673 \\
\SepLayerBN&  0.84758 &   0.5880 &  0.4128 &  1.2095 \\
\bottomrule
\end{tabular}
\end{center}

\caption{Maximal performance results for the \cifar dataset. From left to right, accuracy and loss (for \emph{train} and \emph{validation} sets) for \ReLUBN,  \SepLayer, and \SepLayerBN}
\label{tab:sc-relu-bn}
\end{table}

\subsection{The Effect of Constraint Loss}\label{subsec:constraintLoss}
The constraint loss \ref{eq:constraintloss}, measures the  violation of the constraints \cite{florenzano2001ConvexAnalysis,Burges1998TutorialOnSVMForPatternRecognition}, besides the obvious relation between loss and accuracy \cite{LeCun06atutorial,lecun2015DeepLearningBig}, for the case of \SepUnit, we must notice that the total Loss of training a DNN with \SepUnit (equation \ref{eq:constraintLossFunctional}) linearly combines cross-entropy loss and constraint loss, that constructs a training sequence composed of two gradients, that yields a non-linear combination of the learning rate $\gamma$ and the constraint loss parameter $\lambda$ (a substitution into equation \ref{eq:trainingSequence}), so that the resulting training sequence is of the form.
\begin{equation}\label{eq:resultingConstraintLoss}
\theta_{n+1} = \theta_n - \gamma\nabla_{\theta}\mathcal{L}-\sum_{\xi\in\Xi}\lambda\gamma\nabla\xi.
\end{equation}
Choosing the zero initialization scheme implies choosing $\theta_0=\vec{0}$ or otherwise on the Glorot scheme. As Figure \ref{fig:peaks} showcases, variations in $\xi$ (i.e. constraint loss) have a direct impact on accuracy.  From a geometric point of view, the values of $\xi$ aim to ensure that zero sets of layers are non-empty in their intersection with the training set (a consequence of the topological fact in remark \ref{remark:topologicalFacts}): choosing Glorot initialization (and in a worse case the zero initialization) may yield a violation of both constraints and remark \ref{remark:topologicalFacts} starting the total loss at an arbitrary high value (in Figure \ref{fig:peaks} reaching a value of $79$ in the first epochs).     
\\\\
In addition, Figure \ref{fig:peaks} also showcases the relation between values of the constraint loss in relation to validation accuracy that are significant. After drops in constraint loss, accuracy changes abruptly (see the behavior around iteration 458, where accuracy drops to 21\%), such conclusion is reached also when observing Figures \ref{fig:lambdas} and \ref{fig:constraintLoss} (in the case of \cifar). Increasing values of $\lambda$ reinforces the positive effect that constraint loss has on cross-entropy, and even more so in the presence of zero initialization. However, constraint loss cannot be the sole factor in accuracy increases (notice the abrupt increase in accuracy around iteration $750$). We observe that cross-entropy loss has a much direct impact in overall accuracy, as presented by Figure \ref{fig:peaks}: around iteration $750$, the cross-entropy loss suffers an abrupt change, that increases accuracy until iteration $1750$.   
\\\\
In matters of the influence of Glorot and zero initialization, Figure \ref{fig:zeroConvergence} shows that lower values of $\gamma$ ensure convergence in accuracy. Zero initialization allows the training sequence to reach a higher value (as noted in subsection \ref{subsec:accuracyResults}), but also in less epochs. However, Glorot initialization yields a an almost monotonic behavior of accuracy, in contrast to the \emph{oscillating} behavior observed using both schemes, as parts \ref{fig:glorotWeightGradient} and \ref{fig:zerosWeightGradient} of Figure \ref{fig:histo} suggest. We must recall that the geometric formulation underlying \SepUnit merely guarantees non-empty zero sets and non-empty supports. However, this oscillatory dynamic could constitute evidence of \emph{data shattering} \cite{Zhang2016RethinkingGeneralization}. Still, the role of the \emph{balance} parameter \ref{eq:separatingUnit} which is needed to bias the constraint to the positive side in order to break symmetry requires more research.
\\\\
Upon closer look, the histograms for weights and bias presented in Figure \ref{fig:histo}, suggest that Glorot may lead to data shattering (choosing more \emph{disperse} weights throughout training, as presented in part \ref{fig:glorotWeights}), while the variance of weights under the zero initialization is lower (part \ref{fig:zerosWeights}). From a geometric point of view, the zero initialization leads to \emph{smaller} normal vectors for separating planes. However, as Figures \ref{fig:glorotBias} and \ref{fig:zerosBias} showcase, possible bias  compose a wide range in both initialization schemes: $[-0.6,0.2]$ for Glorot, while its range is $[-0.02,0.18]$. This could contradict the fact that separation \emph{must occur} on the first orthant $\Real{4}$, however without correlations between weights and bias, such conclusion cannot be reached beyond intuition.    
\\\\
From a geometric stand point, this oscillating phenomena has a very intuitive explanation. As points are mapped through the intermediate representations, they sometimes fall into zero set of a layer, while weights and biases are being updated, moving in between zero set and support that are divided via the hyperplane described in equation \ref{eq:supportAndZerosOfUnit}. From the point of view of hyperplanes, this amounts to swapping the signs of normal vectors, sometimes in harmony with the bias, sometimes against it (see iterations 500 to 600 in Figure \ref{fig:peaks}). 
\\\\
In matters of implementation, the two parts of the predicates in the constraint formulation are involved (recall equations \ref{eq:constraintLinearFormulation} and \ref{eq:predicateForSepUnitP}) for the maximum and minimum values of $p$. These induce two constraints, that are void under the zero initialization.  In order to break the tie, we use a convex combination of both constraints biasing them at $0.51$ (towards the side of maximum). So that this small bias \emph{jumpstarts} the network into action.  

\begin{figure}[h]
  \centering  
	\includegraphics[width=0.5\textwidth]{constraint_loss}
      \caption{Scatter plot of constraint loss versus accuracy involving all successful tests with \cifar.}
\label{fig:constraintLoss}
\end{figure}

\begin{figure}[h]
\centering    
\includegraphics[width=0.5\textwidth]{lambdas}
      \caption{Accuracy versus $\lambda$. Left-hand side using Glorot initialization scheme. Right-hand side using zero intialization scheme.}
\label{fig:lambdas}
\end{figure}

\begin{figure*}[h]
  \begin{center}
    \includegraphics[width=1.0\textwidth]{peaks}
      \caption{Evolution of training throughout epochs (cross-entropy, constraint loss, and accuracy). Left-hand axis show the accuracy metric (blue line) against the cross-entropy, and constraint loss in the right axis (orange line), for each epoch of the training phase in the horizontal axis.}
			\label{fig:peaks}
\end{center}
\end{figure*}

\begin{figure*}[h]
  \centering
		\includegraphics[width=1.0\textwidth]{zeros-vs-glorot}
      \caption{Convergence plot for the evolution of the validation accuracy during the training phase. Solid lines stand for Zero initialization, dashed stand for Glorot initialization. The colors encode the different learning rates $\gamma$ used, red for $0.001$, green for $0.0005$ and blue for $0.0001$.}
\label{fig:zeroConvergence}
\end{figure*}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.5\textwidth]{zeros-vs-glorot-val-difference}
      \caption{Histogram of gaps between train and validation accuracy, for Zero and Glorot initialization over all successful runs of \cifar.}
		\label{fig:ZeroVsGlorotDifference}


\end{figure}

\begin{figure} 
    \centering
  \subfloat[Glorot weights\label{fig:glorotWeights}]{%
       \includegraphics[width=0.45\linewidth]{glorot-kernel.png}}
    \hfill
  \subfloat[Zeros weights\label{fig:zerosWeights}]{%
        \includegraphics[width=0.45\linewidth]{zeros-kernel.png}}
    \\
  \subfloat[Glorot weight gradient\label{fig:glorotWeightGradient}]{%
        \includegraphics[width=0.45\linewidth]{glorot-kernel-grad.png}}
    \hfill
  \subfloat[Zeros weight gradient\label{fig:zerosWeightGradient}]{%
        \includegraphics[width=0.45\linewidth]{zeros-kernel-grad.png}}
        
   \subfloat[Glorot bias\label{fig:glorotBias}]{%
       \includegraphics[width=0.45\linewidth]{glorot-bias.png}}
    \hfill
  \subfloat[Zeros bias\label{fig:zerosBias}]{%
        \includegraphics[width=0.45\linewidth]{zeros-bias.png}}
    \\
  \subfloat[Glorot output\label{fig:glorotOutput}]{%
        \includegraphics[width=0.45\linewidth]{glorot-out.png}}
    \hfill
  \subfloat[Zeros output\label{fig:zerosOutput}]{%
        \includegraphics[width=0.45\linewidth]{zeros-out.png}}
  \caption{Histogram displaying weights, biases, gradients, and outputs from the $25$th layer constructed from a \cifar experiment, comparing zero (right-hand side in cyan) and Glorot initialization (left-hand side in magenta).}
  \label{fig:histo} 
\end{figure}
