\section{Separating constraints}\label{sec:constraint}
Conceptually, we wish to enforce \emph{separability} based on the definition provided in equations \ref{eq:predicateWithUpper} and \ref{eq:pointPredicate}. However, set theoretic functions are not differentiable \cite{Glorot10Initialization,lecun2015DeepLearningBig,munkres2000Topology}. Thus, we can make use of the characterization of separability using pre-activations as done in Equation \ref{eq:preactivationCharacterization}. That is, given a fixed choice of weight vector  $\vec{w}\in\Real{n}$ and bias $b\in\mathbb{R}$, we define slack variables $\xi^{+},\xi^{-}\geq 0$ for a dataset $\mathcal{T}$ (via set $X$) as a pair of numbers such that
\begin{equation}\label{eq:basicConstraintFormulation}
\begin{array}{lcl}
    \max\{\vec{w}\cdot\vec{x}+b|\vec{x}\in X\}\geq 1-\xi^{+}\\
    \min\{\vec{w}\cdot\vec{x}+b|\vec{x}\in X\}\leq -1+\xi^{-}\\
\end{array}
\end{equation}
This slack variables can be added to the optimization problem defined in Equation \ref{eq:basicConstraintFormulation}. More specifically, treat it as a multiobjective optimization problem using a technique known as \emph{scalarization} \cite{boyd} (equation \ref{eq:marginOptimizationProblem}) as
\begin{equation}\label{eq:marginOptimizationProblem}
\argmin_{\theta,\xi^{+},\xi^{-}} \mathcal{L}(\mathcal{T},\theta) + \lambda g(\xi^{+},\xi^{-})
\end{equation}
where $\lambda$ is a \emph{hyper-parameter} that introduces a trade-off between the constraint fulfillment and the main task loss. Intuitively, we mean to \emph{balance} the effect of the constraints and solving the actual problem.
\\\\
The intuition behind the introduction of these constraints presented in Equation \ref{eq:basicConstraintFormulation} is as follows. 
\\\\
We introduce the constraints in pair according to the sign. $\xi^{+}$ intends to create at least one pre-activation value greater (or equal) than 1, in order to force the weights and biases to be chosen so that the lower part of the units is non-empty. 
\\\\
In symmetry, enforcing the constraint with $\xi^{-}$ promotes at least pre-activation of points of $X$ below -1. This intends to ensure that the upper parts of the units are non-empty. Notice how the inclusion of the \emph{max} is what carries this effect of \emph{at least one}, and what it makes different from SVM.
\\\\
Thus, since neither the upper, nor the lower part of the unit is empty (i.e. the units are neither dead nor affine in the sense of equations \ref{eq:deadUnitVersion1} and equation \ref{eq:affineUnit} respectively).
\\\\
Additionally, since introducing those constraints imply the the value of the weights now depend on their output instead of only on the loss functional, this enables to use \emph{zero-initialization}. However, notice that since values of $\xi^+$ and $\xi^-$ will be equal and cancel each other, we need to introduce a mechanism to break the tie. In our case we chose a convex combination of both that we name $g$. We use a negligible value (0.51 in practice) so it does not bias the problem.

\begin{equation}\label{eq:definitionOfRho}
    g(\xi^{+},\xi^{-}) = \rho\xi^{+}+(1-\rho)\xi^{-}
\end{equation}

What remains of this section is to choose the object of the constraints among the different structures of a network: by unit and by point.

\subsection{Unit based separation constraints \SepUnit}\label{subsec:sepUnit}

In order to avoid the presence of dead units (recall Equation \ref{eq:defDeadUnit} on Remark \ref{remark:deadItems}), we introduce the \emph{Unit Based Separation Constraint} \SepUnit. Within a \ReLU DNN $F$, choose a unit $u_j^k$ in layer $\layer_k:\Real{n_k}\rightarrow\Real{m_k}$. This unit will be defined univocally by its parameters:
\begin{equation}\label{eq:unitSepParameterWriting}
    u^k_j(\vec{x}) = u(\vec{x};\vec{w}^k_j,b^k_j)
\end{equation}
in order to enforce separation within this unit, we must substitute parameters in equation \ref{eq:basicConstraintFormulation} and create constraints of the form:
\begin{equation}
    \begin{array}{lcl}
    \max\{\vec{w}^k_j\cdot\vec{x}+b^k_j|\vec{x}\in X\}\geq 1-\xi^{+}_{jk}\\
    \min\{\vec{w}^k_j\cdot\vec{x}+b^k_j|\vec{x}\in X\}\leq -1+\xi^{-}_{jk}\\
\end{array}
\end{equation}
for $k=1,\ldots,D$ and $j$ the number of units per layer. As a consequence, we will need to introduce balancing parameters $rho_{jk}$ on function $g$. Individually, we will need to minimize functions of the form
\begin{equation}
    g(\xi^{+},\xi^{-}) = \rho_{jk}\xi^{+}_{jk}+(1-\rho_{jk})\xi^{-}_{jk}
\end{equation}
so that at a global scale we will have to add to the loss functional a term of the form:
\begin{equation}\label{eq:constraintLossForUnitSeparation}
    G = \sum_{k=1}^{D}\sum_{j=1}^{m_k}\rho_{jk}\xi^{+}_{jk}+(1-\rho_{jk})\xi^{-}_{jk}
\end{equation}
We name this the \emph{Unit-based} Separation formulation \SepUnit. 

\subsection{Point Based Separation \SepPoint}\label{subsec:sepPoint}

In order to avoid the presence of dead points (recall again Remark \ref{remark:deadItems} and definition \ref{def:separatingPoint}), we introduce a \emph{Point Based Separation} Constraint \SepPoint. In practice the entire dataset is not used for training, but \emph{batches} of it \cite{LeCun06atutorial}. And small batch size may cause inconsistencies within the training process.

Under our formulation,  these issues would translate to unit penalization for not fulfilling \SepUnit on a \emph{per-batch} basis (due to the fact that the points that comply with the constraints may lie outside the batch).  Thus, it is desirable to make our constraint formulation independent of batch choice by focusing on the values that constraints take on specific points. 
\\\\
That is, we will constraint the slacks $\xi^{\pm}$ to each point on the batch. That is given $\vec{x}\in X$, and $u_1,\ldots,u_m$ unit functions in a layer, we define slack variables $\xi^{-}_{xk},\xi^{+}_{xk}\geq 0$ in the context of the following constraints:
\begin{equation}\label{eq:pointSeparationConstraint}
\begin{array}{lcl}
    \displaystyle\max_{j=1,\ldots,m}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\geq 1-\xi^{+}_{xk}\\\\
    \displaystyle\min_{j=1,\ldots,m_k}\{\vec{w}^j_k\cdot\vec{x}+b^j_k\}\leq -1+\xi^{-}_{xk}\\
\end{array}    
\end{equation}
so that the associated term to minimize will have the following form \ref{eq:convexCombinationOfConstraints}:
\begin{equation}\label{eq:convexCombinationOfConstraints}
    G = \sum_{k=1}^{D}\rho_{k}\xi^{+}_{xk}+(1-\rho_{k})\xi^{-}_{xk}
\end{equation}

Notice that \SepPoint does not prevent  dead units (as \SepUnit). We can combine both into \SepUnitPoint, to ensure that neither dead points nor dead units are present. 